{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f2bda56",
   "metadata": {},
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17919e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Lab\\Research\\EMERGE-REPLICATE\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 30 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:00<00:00, 30016.49it/s]\n",
      "Some weights of LongformerModel were not initialized from the model checkpoint at yikuan8/Clinical-Longformer and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n",
      "model device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import  os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "#\n",
    "from utils.bgem3 import cosine_filter, batch_encode\n",
    "from utils.call_llm import extract_note, create_summary\n",
    "from utils.clinical_longformer import langchain_chunk_embed, plain_truncate\n",
    "from utils.constants import *\n",
    "#\n",
    "import torch\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "#\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict, Counter\n",
    "import pickle\n",
    "import h5py\n",
    "#\n",
    "from tqdm import tqdm\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28e8d0b",
   "metadata": {},
   "source": [
    "# Part 1: Preprocess EHR-Notes Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65743928",
   "metadata": {},
   "source": [
    "## I. Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264d45b3",
   "metadata": {},
   "source": [
    "### Keep only first episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "652b6ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ngbac\\AppData\\Local\\Temp\\ipykernel_6780\\2013730235.py:9: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['10000' '10000' '10000' ... '9' '9' '9']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  clean.loc[:, \"PatientID\"] = extracted.loc[keep_mask, \"base\"].astype(str)\n"
     ]
    }
   ],
   "source": [
    "def clean_patient_ids(INPUT_CSV, OUTPUT_CSV):\n",
    "    df = pd.read_csv(INPUT_CSV, low_memory=False, encoding=\"utf-8\")\n",
    "    pid = df[\"PatientID\"].astype(str)\n",
    "    extracted = pid.str.extract(r\"^(?P<base>\\d+)(?:_(?P<suf>\\d+))?$\")\n",
    "\n",
    "    suf_num = pd.to_numeric(extracted[\"suf\"], errors=\"coerce\")\n",
    "    keep_mask = suf_num.isna() | (suf_num == 1)\n",
    "    clean = df.loc[keep_mask].copy()\n",
    "    clean.loc[:, \"PatientID\"] = extracted.loc[keep_mask, \"base\"].astype(str)\n",
    "    clean.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "clean_patient_ids(EHR_ZHU, EHR_BACH)\n",
    "clean_patient_ids(NOTES_ZHU, NOTES_BACH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a904ca",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9724ed74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patients with conflicting Outcome or Readmission values:\n",
      "Empty DataFrame\n",
      "Columns: [Outcome, Readmission]\n",
      "Index: []\n",
      "Distribution of records per patient:\n",
      "1 record(s): 970 patients\n",
      "2 record(s): 6347 patients\n",
      "3 record(s): 5820 patients\n",
      "4 record(s): 4703 patients\n",
      "5 record(s): 2934 patients\n",
      "6 record(s): 2510 patients\n",
      "7 record(s): 1598 patients\n",
      "8 record(s): 1305 patients\n",
      "9 record(s): 901 patients\n",
      "10 record(s): 871 patients\n",
      "\n",
      "Cumulative distribution (>= records):\n",
      ">= 1 record(s): 33469 patients\n",
      ">= 2 record(s): 32499 patients\n",
      ">= 3 record(s): 26152 patients\n",
      ">= 4 record(s): 20332 patients\n",
      ">= 5 record(s): 15629 patients\n",
      ">= 6 record(s): 12695 patients\n",
      ">= 7 record(s): 10185 patients\n",
      ">= 8 record(s): 8587 patients\n",
      ">= 9 record(s): 7282 patients\n",
      ">= 10 record(s): 6381 patients\n"
     ]
    }
   ],
   "source": [
    "notes = pd.read_csv(NOTES_BACH, dtype={\"PatientID\": \"string\"}, encoding=\"utf-8\", low_memory=False)\n",
    "ehr   = pd.read_csv(EHR_BACH,   dtype={\"PatientID\": \"string\"}, encoding=\"utf-8\", low_memory=False)\n",
    "\n",
    "# Check for patients with conflicting Outcome or Readmission values\n",
    "conflict = ehr.groupby(\"PatientID\")[[\"Outcome\", \"Readmission\"]].nunique()\n",
    "conflict_patients = conflict[(conflict[\"Outcome\"] > 1) | (conflict[\"Readmission\"] > 1)]\n",
    "\n",
    "print(\"Patients with conflicting Outcome or Readmission values:\")\n",
    "print(conflict_patients)\n",
    "\n",
    "dist = (\n",
    "    ehr.groupby(\"PatientID\").size()      # rows per patient\n",
    "       .value_counts()                   # how many patients have that many rows\n",
    "       .sort_index()                     # sort by row count\n",
    ")\n",
    "\n",
    "print(\"Distribution of records per patient:\")\n",
    "for rows, n_patients in dist.items():\n",
    "    print(f\"{rows} record(s): {n_patients} patients\")\n",
    "    if rows >= 10:\n",
    "        break\n",
    "\n",
    "dist_ge = dist.sort_index(ascending=False).cumsum().sort_index()\n",
    "print(\"\\nCumulative distribution (>= records):\")\n",
    "for rows, n_patients in dist_ge.items():\n",
    "    print(f\">= {rows} record(s): {n_patients} patients\")\n",
    "    if rows >= 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "000dc6d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PatientID</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000</td>\n",
       "      <td>11:30 am chest ( portable ap ) clip # reason :...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10003</td>\n",
       "      <td>admission note d : pt arrived from , sedated o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10004</td>\n",
       "      <td>respiratory care pt was admitted today from os...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10006</td>\n",
       "      <td>full code universal precautions allergy : hepa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10007</td>\n",
       "      <td>12:24 pm chest port . line placement clip # re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  PatientID                                               Text\n",
       "0     10000  11:30 am chest ( portable ap ) clip # reason :...\n",
       "1     10003  admission note d : pt arrived from , sedated o...\n",
       "2     10004  respiratory care pt was admitted today from os...\n",
       "3     10006  full code universal precautions allergy : hepa...\n",
       "4     10007  12:24 pm chest port . line placement clip # re..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine all notes for each patient into a single string, separated by \"\\n\"\n",
    "notes = (\n",
    "    notes\n",
    "    .groupby(\"PatientID\", sort=False)[\"Text\"]\n",
    "    .apply(lambda s: \"\\n\".join(s.astype(str)))\n",
    "    .reset_index(name=\"Text\")\n",
    ")\n",
    "\n",
    "notes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6265c3f0",
   "metadata": {},
   "source": [
    "#### Method 1: Remove all patients with less than X records, and keep only X records among those qualified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d23155b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Limited to first {RECORDS} EHR rows per patient.\")\n",
    "\n",
    "# ehr_counts = ehr.groupby(\"PatientID\").size()\n",
    "# valid_ehr_ids = ehr_counts[ehr_counts >= RECORDS].index\n",
    "# removed_patients = set(ehr[\"PatientID\"].unique()) - set(valid_ehr_ids)\n",
    "# ehr = ehr[ehr[\"PatientID\"].isin(valid_ehr_ids)].reset_index(drop=True)\n",
    "\n",
    "# print(f\"Patients removed for having < {RECORDS} EHR rows: {len(removed_patients)}\")\n",
    "\n",
    "# ehr = (\n",
    "#     ehr\n",
    "#     .groupby(\"PatientID\", group_keys=False)\n",
    "#     .head(RECORDS)\n",
    "#     .reset_index(drop=True)\n",
    "# )\n",
    "\n",
    "# print(f\"EHR rows after truncation: {len(ehr):,}\")\n",
    "# print(f\"EHR patients: {ehr['PatientID'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6692af",
   "metadata": {},
   "source": [
    "#### Method 2: Same as method 1, but we respect the RecordTime (take up to RecordTime X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5289197c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Keep a copy for reporting\n",
    "# _ids_before = set(ehr[\"PatientID\"].unique())\n",
    "# _rows_before = len(ehr)\n",
    "\n",
    "# # Coerce RecordTime to numeric and drop invalid/missing\n",
    "# ehr[\"RecordTime\"] = pd.to_numeric(ehr[\"RecordTime\"], errors=\"coerce\")\n",
    "# ehr = ehr[ehr[\"RecordTime\"].between(1, RECORDS, inclusive=\"both\")].copy()\n",
    "\n",
    "# # Keep only patients that have all RecordTime values 1..RECORDS present\n",
    "# rt_counts = ehr.groupby(\"PatientID\")[\"RecordTime\"].nunique()\n",
    "# valid_ids = rt_counts[rt_counts == RECORDS].index\n",
    "\n",
    "# removed_patients = _ids_before - set(valid_ids)\n",
    "# ehr = ehr[ehr[\"PatientID\"].isin(valid_ids)].sort_values([\"PatientID\", \"RecordTime\"]).reset_index(drop=True)\n",
    "\n",
    "# print(f\"Rows before: {_rows_before:,} | after filtering by RecordTime: {len(ehr):,}\")\n",
    "# print(f\"Patients removed for missing any RecordTime in 1..{RECORDS}: {len(removed_patients):,}\")\n",
    "# print(f\"EHR rows kept: {len(ehr):,}\")\n",
    "# print(f\"EHR patients kept: {ehr['PatientID'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcaa648",
   "metadata": {},
   "source": [
    "### Method 3: keep all patients, add empty rows for those who don't have enough 48 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "feddf63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targeting exactly 48 rows per patient (pad with empties; RecordTime=-1).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ngbac\\AppData\\Local\\Temp\\ipykernel_6780\\2403649650.py:25: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(pad_or_trim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patients: 33,469\n",
      "Rows after pad/trim: 1,606,512 (expected 1,606,512)\n",
      "Padded rows added: 1,364,295\n"
     ]
    }
   ],
   "source": [
    "id_col   = \"PatientID\"\n",
    "time_col = \"RecordTime\"\n",
    "keep_dup = [\"Outcome\", \"Readmission\", \"Sex\", \"Age\"]\n",
    "\n",
    "print(f\"Targeting exactly {RECORDS} rows per patient (pad with empties; {time_col}=-1).\")\n",
    "\n",
    "def pad_or_trim(g):\n",
    "    g = g.sort_values(time_col, kind=\"stable\").head(RECORDS)\n",
    "    missing = RECORDS - len(g)\n",
    "    if missing <= 0:\n",
    "        return g\n",
    "\n",
    "    # Build a template \"empty\" row\n",
    "    base = {c: np.nan for c in ehr.columns}\n",
    "    base[id_col] = g[id_col].iat[0]\n",
    "    for c in keep_dup:\n",
    "        if c in g: base[c] = g[c].iat[0]\n",
    "    base[time_col] = -1  # negative so it won't look like a latest record\n",
    "\n",
    "    filler = pd.DataFrame([base] * missing, columns=ehr.columns)\n",
    "    return pd.concat([g, filler], ignore_index=True)\n",
    "\n",
    "ehr = (\n",
    "    ehr.groupby(id_col, group_keys=False)\n",
    "       .apply(pad_or_trim)\n",
    "       .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "n_patients = ehr[id_col].nunique()\n",
    "n_rows     = len(ehr)\n",
    "n_pad      = int((ehr[time_col] == -1).sum())\n",
    "\n",
    "print(f\"Patients: {n_patients:,}\")\n",
    "print(f\"Rows after pad/trim: {n_rows:,} (expected {n_patients*RECORDS:,})\")\n",
    "print(f\"Padded rows added: {n_pad:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f776e3f",
   "metadata": {},
   "source": [
    "### Keep only patients with both ehr and notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b75076ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BEFORE ===\n",
      "Notes: 31,074 rows | 31,074 unique patients\n",
      "EHR  : 1,606,512 rows | 33,469 unique patients\n",
      "\n",
      "=== AFTER (kept only patients present in BOTH) ===\n",
      "Common patients kept: 31,027\n",
      "Notes.filtered.csv: 31,027 rows | 31,027 patients\n",
      "ehr.filtered.csv  : 1,489,296 rows | 31,027 patients\n"
     ]
    }
   ],
   "source": [
    "# normalize IDs a bit (handle stray spaces / empty)\n",
    "notes[\"PatientID\"] = notes[\"PatientID\"].str.strip()\n",
    "ehr[\"PatientID\"]   = ehr[\"PatientID\"].str.strip()\n",
    "\n",
    "# ---- find intersection ----\n",
    "ids_notes = set(notes[\"PatientID\"].unique())\n",
    "ids_ehr   = set(ehr[\"PatientID\"].unique())\n",
    "ids_both  = ids_notes & ids_ehr\n",
    "\n",
    "# ---- filter to the same set ----\n",
    "notes_f = notes[notes[\"PatientID\"].isin(ids_both)].copy()\n",
    "ehr_f   = ehr[ehr[\"PatientID\"].isin(ids_both)].copy()\n",
    "\n",
    "# ---- sort for readability ----\n",
    "notes_f = notes_f.sort_values([\"PatientID\"]).reset_index(drop=True)\n",
    "ehr_f   = ehr_f.sort_values([\"PatientID\", \"RecordTime\"]).reset_index(drop=True)\n",
    "\n",
    "ehr_f.drop(columns=[\"RecordTime\"], inplace=True)\n",
    "\n",
    "# ---- save ----\n",
    "notes_f.to_csv(NOTES_BACH, index=False)\n",
    "ehr_f.to_csv(EHR_BACH,   index=False)\n",
    "\n",
    "# ---- report ----\n",
    "print(\"=== BEFORE ===\")\n",
    "print(f\"Notes: {len(notes):,} rows | {len(ids_notes):,} unique patients\")\n",
    "print(f\"EHR  : {len(ehr):,} rows | {len(ids_ehr):,} unique patients\")\n",
    "print(\"\\n=== AFTER (kept only patients present in BOTH) ===\")\n",
    "print(f\"Common patients kept: {len(ids_both):,}\")\n",
    "print(f\"Notes.filtered.csv: {len(notes_f):,} rows | {notes_f['PatientID'].nunique():,} patients\")\n",
    "print(f\"ehr.filtered.csv  : {len(ehr_f):,} rows | {ehr_f['PatientID'].nunique():,} patients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dea3991",
   "metadata": {},
   "source": [
    "### Splitting Train/Test/Val sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c9c22d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient counts per joint class (O*2+R): Counter({0: 26775, 3: 3186, 1: 1015, 2: 51})\n",
      "Train/Val/Test patients: 21718 3103 6206\n",
      "Train: rows=1,042,464, patients=21,718\n",
      "Val: rows=148,944, patients=3,103\n",
      "Test: rows=297,888, patients=6,206\n",
      "Train ‚Äî Outcome: rows=10.434%, patients=10.434% | Readmission: rows=13.542%, patients=13.542%NaN: 0\n",
      "Val ‚Äî Outcome: rows=10.442%, patients=10.442% | Readmission: rows=13.535%, patients=13.535%NaN: 0\n",
      "Test ‚Äî Outcome: rows=10.425%, patients=10.425% | Readmission: rows=13.535%, patients=13.535%NaN: 0\n"
     ]
    }
   ],
   "source": [
    "def cols_between(_df, start_label, end_label=None):\n",
    "    cols = _df.columns\n",
    "    start_idx = cols.get_loc(start_label)\n",
    "    end_idx = len(cols) - 1 if end_label is None else cols.get_loc(end_label)\n",
    "    if start_idx > end_idx:\n",
    "        raise ValueError(f\"{start_label!r} comes after {end_label!r} in columns\")\n",
    "    return cols[start_idx:end_idx + 1]\n",
    "\n",
    "def data_split_and_impute():\n",
    "    df = pd.read_csv(EHR_BACH, encoding='utf-8', low_memory=False)\n",
    "\n",
    "    pat = (\n",
    "        df.groupby('PatientID', as_index=False)\n",
    "            .agg(Outcome=('Outcome','first'),\n",
    "                Readmission=('Readmission','first'))\n",
    "    )\n",
    "    pat['joint'] = pat['Outcome'].astype(int)*2 + pat['Readmission'].astype(int)\n",
    "    print(\"Patient counts per joint class (O*2+R):\", Counter(pat['joint']))\n",
    "\n",
    "    # 1) TEST = 20% (stratified on joint)\n",
    "    pat_trainval, pat_test = train_test_split(\n",
    "        pat, test_size=0.20, stratify=pat['joint'], random_state=RANDOM_STATE\n",
    "    )\n",
    "\n",
    "    # 2) VAL = 12.5% of remaining (i.e., ~10% overall)\n",
    "    pat_train, pat_val = train_test_split(\n",
    "        pat_trainval, test_size=0.125, stratify=pat_trainval['joint'], random_state=RANDOM_STATE\n",
    "    )\n",
    "\n",
    "    print(\"Train/Val/Test patients:\", len(pat_train), len(pat_val), len(pat_test))\n",
    "\n",
    "    train_ids = set(pat_train['PatientID'])\n",
    "    val_ids   = set(pat_val['PatientID'])\n",
    "    test_ids  = set(pat_test['PatientID'])\n",
    "\n",
    "    train_df = df[df['PatientID'].isin(train_ids)].copy()\n",
    "    val_df   = df[df['PatientID'].isin(val_ids)].copy()\n",
    "    test_df  = df[df['PatientID'].isin(test_ids)].copy()\n",
    "\n",
    "    for name, d in [('Train', train_df), ('Val', val_df), ('Test', test_df)]:\n",
    "        print(f\"{name}: rows={len(d):,}, patients={d['PatientID'].nunique():,}\")\n",
    "\n",
    "    # ====== DEFINE FEATURE BLOCKS ======\n",
    "    cat_cols = list(cols_between(\n",
    "        df,\n",
    "        \"Capillary refill rate->0.0\",\n",
    "        \"Glascow coma scale verbal response->3 Inapprop words\"\n",
    "    ))\n",
    "    num_cols = list(cols_between(df, \"Diastolic blood pressure\", None))\n",
    "\n",
    "    # ====== COMPUTE IMPUTATION VALUES ON TRAIN ONLY ======\n",
    "    # Categorical: all NaNs -> 0 (no stats needed)\n",
    "    # Numeric: per-column mean from TRAIN\n",
    "    # Ensure numeric dtype for means; if some numeric cols are object due to bad parsing, coerce safely\n",
    "    train_num = train_df[num_cols].apply(pd.to_numeric, errors='coerce')\n",
    "    num_impute = train_num.mean()  # pandas Series indexed by column name\n",
    "\n",
    "    # ====== APPLY IMPUTATION (USING TRAIN STATS) ======\n",
    "    def apply_impute(d):\n",
    "        d = d.copy()\n",
    "        if cat_cols:\n",
    "            d.loc[:, cat_cols] = d.loc[:, cat_cols].fillna(0)\n",
    "        if num_cols:\n",
    "            d.loc[:, num_cols] = d.loc[:, num_cols].apply(pd.to_numeric, errors='coerce')\n",
    "            d.loc[:, num_cols] = d.loc[:, num_cols].fillna(num_impute)\n",
    "        return d\n",
    "\n",
    "    train_df_i = apply_impute(train_df)\n",
    "    val_df_i   = apply_impute(val_df)\n",
    "    test_df_i  = apply_impute(test_df)\n",
    "\n",
    "    # ====== QUICK CHECKS AFTER IMPUTATION ======\n",
    "    def summarize_split(name, df_rows, df_pat):\n",
    "        o_row = df_rows['Outcome'].mean()\n",
    "        r_row = df_rows['Readmission'].mean()\n",
    "        o_pat = df_pat['Outcome'].mean()\n",
    "        r_pat = df_pat['Readmission'].mean()\n",
    "        print(\n",
    "            f\"{name} ‚Äî Outcome: rows={o_row:.3%}, patients={o_pat:.3%} | \"\n",
    "            f\"Readmission: rows={r_row:.3%}, patients={r_pat:.3%}\"\n",
    "            f\"NaN: {(df_rows.isna().sum().sum())}\"\n",
    "        )\n",
    "\n",
    "    summarize_split(\"Train\", train_df_i, pat_train)\n",
    "    summarize_split(\"Val\",   val_df_i,   pat_val)\n",
    "    summarize_split(\"Test\",  test_df_i,  pat_test)\n",
    "\n",
    "    train_df_i.to_csv(TRAIN_DRAFT, index=False)\n",
    "    val_df_i.to_csv(VAL_DRAFT, index=False)\n",
    "    test_df_i.to_csv(TEST_DRAFT, index=False)\n",
    "\n",
    "if not os.path.exists(TRAIN_DRAFT) or not os.path.exists(VAL_DRAFT) or not os.path.exists(TEST_DRAFT):\n",
    "    data_split_and_impute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1177da",
   "metadata": {},
   "source": [
    "## II. Check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf9c213",
   "metadata": {},
   "source": [
    "### Check Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cecaad6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üåç Overall stats:\n",
      "{'max_chars': 52570, 'avg_chars': np.float64(7748.050439939408), 'max_words': 9521, 'avg_words': np.float64(1474.1109034067103)}\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(NOTES_BACH, encoding=\"utf-8\", low_memory=False)\n",
    "\n",
    "def char_count(text):\n",
    "    return len(text)\n",
    "def word_count(text):\n",
    "    return len(text.split())\n",
    "\n",
    "df[\"char_count\"] = df[\"Text\"].astype(str).apply(char_count)\n",
    "df[\"word_count\"] = df[\"Text\"].astype(str).apply(word_count)\n",
    "\n",
    "overall = {\n",
    "    \"max_chars\": df[\"char_count\"].max(),\n",
    "    \"avg_chars\": df[\"char_count\"].mean(),\n",
    "    \"max_words\": df[\"word_count\"].max(),\n",
    "    \"avg_words\": df[\"word_count\"].mean(),\n",
    "}\n",
    "\n",
    "print(\"\\nüåç Overall stats:\")\n",
    "print(overall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317e924d",
   "metadata": {},
   "source": [
    "### Check EHR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb62e9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1489296, 64)\n",
      "Outcome positive ratio (each episode): 0.1043284880910175\n",
      "Readmission positive ratio (each episode): 0.13539820156637766\n",
      "===\n",
      "Outcome positive ratio (each patient): 0.1043284880910175\n",
      "Readmission positive ratio (each patient): 0.13539820156637766\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(EHR_BACH, encoding=\"utf-8\", low_memory=False)\n",
    "print(\"Shape:\", df.shape)\n",
    "\n",
    "print(\"Outcome positive ratio (each episode):\", df[\"Outcome\"].mean())\n",
    "print(\"Readmission positive ratio (each episode):\", df[\"Readmission\"].mean())\n",
    "\n",
    "print(\"===\")\n",
    "pat_any = df.groupby(\"PatientID\")[[\"Outcome\",\"Readmission\"]].max()\n",
    "\n",
    "print(\"Outcome positive ratio (each patient):\", pat_any[\"Outcome\"].mean())\n",
    "print(\"Readmission positive ratio (each patient):\", pat_any[\"Readmission\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a38ff03",
   "metadata": {},
   "source": [
    "# Part 2: Preprocess PrimeKG and Extract Entities from Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c50cb7",
   "metadata": {},
   "source": [
    "## PrimeKG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef3ac589",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 30 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = BGEM3FlagModel(\"BAAI/bge-m3\", use_fp16=True, device=device, trust_remote_code=True)\n",
    "\n",
    "def create_adj():\n",
    "    # Create adjacency list\n",
    "    df = pd.read_csv(PRIMEKG_KG, low_memory=False, encoding=\"utf-8\")\n",
    "    df = df[[\"relation\", \"x_index\", \"y_index\"]]\n",
    "\n",
    "    adj_list = defaultdict(list)\n",
    "    for u, v, r in tqdm(zip(df[\"x_index\"].values, df[\"y_index\"].values, df[\"relation\"].values), total=len(df), desc=\"Creating adjacency list\"):\n",
    "        adj_list[int(u)].append((int(v), str(r)))\n",
    "\n",
    "    with open(KG_ADJACENCY, \"wb\") as f:\n",
    "        pickle.dump(adj_list, f, protocol=5)\n",
    "\n",
    "def create_disease_features():\n",
    "    # Create disease feature\n",
    "    df = pd.read_csv(PRIMEKG_DISEASE, low_memory=False, encoding=\"utf-8\")\n",
    "    df = df.sort_values(\"node_index\").reset_index(drop=True)\n",
    "\n",
    "    df[\"Diseases\"] = (\n",
    "        \"[disease name]\" + df[\"mondo_name\"].fillna(\"\") + \" \" +\n",
    "        \"[definition]\" + df[\"mondo_definition\"].combine_first(df[\"orphanet_definition\"]).fillna(\"\") + \" \" +\n",
    "        \"[description]\" + df[\"umls_description\"].fillna(\"\")\n",
    "    )\n",
    "    df[\"embed\"] = list(batch_encode(df[\"Diseases\"].tolist(), batch_size=64, max_length=8192).cpu().numpy())\n",
    "    df = df[[\"node_index\", \"mondo_name\", \"Diseases\", \"embed\"]]\n",
    "\n",
    "    with open(DISEASE_FEATURES, \"wb\") as f:\n",
    "        pickle.dump(df, f, protocol=5)\n",
    "\n",
    "def create_notes_embeddings():\n",
    "    # Embed notes and save to HDF5\n",
    "    with h5py.File(NOTES_EMBEDDINGS, \"w\") as f: # reset file\n",
    "        pass\n",
    "\n",
    "    notes_df = pd.read_csv(NOTES_BACH, encoding=\"utf-8\", low_memory=False)\n",
    "    for idx, row in tqdm(notes_df.iterrows(), total=len(notes_df), desc=\"Embedding notes and saving to HDF5\"):\n",
    "        patient_id = row[\"PatientID\"]\n",
    "        text = row[\"Text\"]\n",
    "        \n",
    "        with h5py.File(NOTES_EMBEDDINGS, \"a\") as h5:\n",
    "            grp = h5.create_group(str(patient_id))\n",
    "            grp.create_dataset(\"PatientID\", data=np.asarray(patient_id, dtype=\"int64\"))\n",
    "            grp.create_dataset(\"Note\", data=langchain_chunk_embed(text), compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ec2b402",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(KG_ADJACENCY):\n",
    "    create_adj()\n",
    "\n",
    "if not os.path.exists(DISEASE_FEATURES):\n",
    "    create_disease_features()\n",
    "\n",
    "if not os.path.exists(NOTES_EMBEDDINGS):\n",
    "    create_notes_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8afd3f5",
   "metadata": {},
   "source": [
    "## Extract Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08db9c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31027\n"
     ]
    }
   ],
   "source": [
    "with open(DISEASE_FEATURES, \"rb\") as f:\n",
    "    kg = pickle.load(f)\n",
    "mapping = dict(zip(kg[\"node_index\"], kg[\"mondo_name\"]))\n",
    "\n",
    "with open(KG_ADJACENCY, \"rb\") as f:\n",
    "    adj = pickle.load(f)\n",
    "\n",
    "notes_emb = {}\n",
    "with h5py.File(NOTES_EMBEDDINGS, \"r\") as h5:\n",
    "    for patient_id in h5.keys():  # each group is named by patient_id\n",
    "        grp = h5[patient_id]\n",
    "        pid = int(grp[\"PatientID\"][()])\n",
    "        embedding = np.array(grp[\"Note\"])\n",
    "        notes_emb[pid] = embedding\n",
    "print(len(notes_emb.keys()))\n",
    "# print(notes_emb[91199].shape) # 768\n",
    "\n",
    "def _to_float32_array(x):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x.detach().cpu().float().numpy()\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return x.astype(\"float32\", copy=False)\n",
    "    raise TypeError(f\"Expected tensor/ndarray, got {type(x)}\")\n",
    "\n",
    "def store_patient(h5_path, p, ehr, target, notes, summary):\n",
    "    with h5py.File(h5_path, \"a\") as h5:\n",
    "        grp = h5.create_group(str(p))\n",
    "        grp.create_dataset(\"PatientID\", data=np.asarray(p, dtype=\"int64\"))\n",
    "        grp.create_dataset(\"X\", data=ehr, compression=\"gzip\")\n",
    "        grp.create_dataset(\"Note\", data=_to_float32_array(notes), compression=\"gzip\")\n",
    "        grp.create_dataset(\"Summary\", data=_to_float32_array(summary), compression=\"gzip\")\n",
    "        grp.create_dataset(\"Y\", data=np.asarray(target, dtype=\"int8\"))\n",
    "\n",
    "def create_dataset(mode = \"train\"):\n",
    "    if mode not in [\"train\", \"val\", \"test\"]:\n",
    "        raise ValueError(\"mode must be 'train', 'val', or 'test'\")\n",
    "    if mode == \"train\":\n",
    "        df = pd.read_csv(TRAIN_DRAFT, encoding=\"utf-8\", low_memory=False)\n",
    "        h5_path = TRAIN\n",
    "        with h5py.File(TRAIN, \"w\") as f:\n",
    "            pass\n",
    "    elif mode == \"val\":\n",
    "        df = pd.read_csv(VAL_DRAFT, encoding=\"utf-8\", low_memory=False)\n",
    "        h5_path = VAL\n",
    "        with h5py.File(VAL, \"w\") as f:\n",
    "            pass\n",
    "    else:\n",
    "        df = pd.read_csv(TEST_DRAFT, encoding=\"utf-8\", low_memory=False)\n",
    "        h5_path = TEST\n",
    "        with h5py.File(TEST, \"w\") as f:\n",
    "            pass\n",
    "    \n",
    "    cat_cols = list(cols_between(\n",
    "        df,\n",
    "        \"Capillary refill rate->0.0\",\n",
    "        \"Glascow coma scale verbal response->3 Inapprop words\"\n",
    "    ))\n",
    "    num_cols = list(cols_between(df, \"Diastolic blood pressure\", None))\n",
    "\n",
    "    col_mean = df[num_cols].mean()\n",
    "    col_std = df[num_cols].std()\n",
    "\n",
    "    entities = defaultdict(list)\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=f\"Processing {mode} data\"):\n",
    "        record = \"\"\n",
    "        PatientID = row[\"PatientID\"]\n",
    "\n",
    "        if row[\"Sex\"] == 1:\n",
    "            record += \"Gender: Male\\n\"\n",
    "        else:\n",
    "            record += \"Gender: Female\\n\"\n",
    "        record += f\"Age: {row['Age']}\\n\"\n",
    "        \n",
    "        for c in cat_cols:\n",
    "            if row[c] == 1:\n",
    "                cat = c\n",
    "                if \"Glascow coma scale total\" not in cat:\n",
    "                    for i in range(0, 30, 1):\n",
    "                        cat = cat.replace(f\"->{i}.0\", \" : \")\n",
    "                        cat = cat.replace(f\"->{i}\", \" : \")\n",
    "                cat = cat.replace(\"->\", \" : \")\n",
    "                entities[PatientID].append(cat)\n",
    "        \n",
    "        for c in num_cols:\n",
    "            if math.isnan(row[c]):\n",
    "                continue\n",
    "            z_score = (row[c] - col_mean[c]) / col_std[c]\n",
    "            if z_score > 2:\n",
    "                entities[PatientID].append(f\"{c} too high\")\n",
    "            elif z_score < -2:\n",
    "                entities[PatientID].append(f\"{c} too low\")\n",
    "\n",
    "    # Match entities to knowledge graph\n",
    "    patients = list(df[\"PatientID\"].unique())\n",
    "\n",
    "    def get_summary(p):\n",
    "        entities[p] = list(set(entities[p]))\n",
    "        summary_entities = \"\"\n",
    "        summary_nodes = \"\"\n",
    "        summary_edges = \"\"\n",
    "        nodes = []\n",
    "        for e in entities[p]:\n",
    "            summary_entities += e + \", \"\n",
    "            idx = cosine_filter(None, e, threshold=0.6, top_k=3)\n",
    "            nodes.extend(idx)\n",
    "        summary_entities = summary_entities[:-2]\n",
    "\n",
    "        nodes = list(set(nodes))\n",
    "        \n",
    "        for n in nodes:\n",
    "            summary_nodes += kg.iloc[n][\"Diseases\"] + \", \"\n",
    "            node_x = kg.iloc[n][\"node_index\"]\n",
    "            for connect_to in adj[n]:\n",
    "                rela = connect_to[1]\n",
    "                node_y = connect_to[0]\n",
    "                if node_y not in kg[\"node_index\"].values:\n",
    "                    continue\n",
    "                e = \"(\" + mapping[node_x] + \", \" + str(rela) + \", \" + mapping[node_y] + \")\"\n",
    "                # print(e)\n",
    "                summary_edges += e + \", \"\n",
    "        summary_edges = summary_edges[:-2]\n",
    "        summary_nodes = summary_nodes[:-2]\n",
    "        summary_notes = extract_note(notes=notes_emb[p])\n",
    "        summary = create_summary(summary_entities, summary_notes, summary_nodes, summary_edges)\n",
    "        return langchain_chunk_embed(summary)\n",
    "\n",
    "    # summaries = defaultdict(list)\n",
    "    # for p in tqdm(patients, total=len(patients), desc=\"Generating summaries\"):\n",
    "    #     summaries[p] = get_summary(p)\n",
    "\n",
    "    feature_cols = [c for c in df.columns if c not in [\"PatientID\",\"Outcome\",\"Readmission\"]]\n",
    "    target_map = df.groupby(\"PatientID\")[[\"Outcome\",\"Readmission\"]].first()\n",
    "\n",
    "    for p in tqdm(patients, total=len(patients), desc=f\"Storing {mode} data to HDF5\"):\n",
    "        data_ehr = df.loc[df[\"PatientID\"] == p, feature_cols].to_numpy()\n",
    "        data_notes = notes_emb[p]\n",
    "        data_summary = data_notes  # default to notes if summary fails\n",
    "        # data_summary = summaries[p]\n",
    "        outcome, readm = target_map.loc[p].astype(int)\n",
    "        data_target = (int(outcome), int(readm))\n",
    "        store_patient(h5_path, p, data_ehr, data_target, data_notes, data_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fdd1a876",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1042464/1042464 [01:45<00:00, 9883.10it/s] \n",
      "Storing train data to HDF5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21718/21718 [46:59<00:00,  7.70it/s]  \n",
      "Processing val data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 148944/148944 [00:17<00:00, 8502.45it/s]\n",
      "Storing val data to HDF5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3103/3103 [01:55<00:00, 26.98it/s]\n",
      "Processing test data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 297888/297888 [00:45<00:00, 6506.24it/s]\n",
      "Storing test data to HDF5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6206/6206 [05:12<00:00, 19.88it/s]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(TRAIN) or not os.path.exists(VAL) or not os.path.exists(TEST):\n",
    "    create_dataset(mode=\"train\")\n",
    "    create_dataset(mode=\"val\")\n",
    "    create_dataset(mode=\"test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
