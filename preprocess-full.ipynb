{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f2bda56",
   "metadata": {},
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17919e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Lab\\Research\\EMERGE-REPLICATE\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 30 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:00<?, ?it/s]\n",
      "Some weights of LongformerModel were not initialized from the model checkpoint at yikuan8/Clinical-Longformer and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n",
      "model device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import  os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "#\n",
    "from utils.bgem3 import cosine_filter, batch_encode\n",
    "from utils.call_llm import extract_note, create_summary\n",
    "from utils.clinical_longformer import langchain_chunk_embed, plain_truncate\n",
    "from utils.constants import *\n",
    "#\n",
    "import torch\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "#\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict, Counter\n",
    "import pickle\n",
    "import h5py\n",
    "#\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28e8d0b",
   "metadata": {},
   "source": [
    "# Part 1: Preprocess EHR-Notes Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65743928",
   "metadata": {},
   "source": [
    "## I. Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264d45b3",
   "metadata": {},
   "source": [
    "### Keep only first episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "652b6ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ngbac\\AppData\\Local\\Temp\\ipykernel_4988\\2013730235.py:9: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['10000' '10000' '10000' ... '9' '9' '9']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  clean.loc[:, \"PatientID\"] = extracted.loc[keep_mask, \"base\"].astype(str)\n"
     ]
    }
   ],
   "source": [
    "def clean_patient_ids(INPUT_CSV, OUTPUT_CSV):\n",
    "    df = pd.read_csv(INPUT_CSV, low_memory=False, encoding=\"utf-8\")\n",
    "    pid = df[\"PatientID\"].astype(str)\n",
    "    extracted = pid.str.extract(r\"^(?P<base>\\d+)(?:_(?P<suf>\\d+))?$\")\n",
    "\n",
    "    suf_num = pd.to_numeric(extracted[\"suf\"], errors=\"coerce\")\n",
    "    keep_mask = suf_num.isna() | (suf_num == 1)\n",
    "    clean = df.loc[keep_mask].copy()\n",
    "    clean.loc[:, \"PatientID\"] = extracted.loc[keep_mask, \"base\"].astype(str)\n",
    "    clean.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "clean_patient_ids(EHR_ZHU, EHR_BACH)\n",
    "clean_patient_ids(NOTES_ZHU, NOTES_BACH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a904ca",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9724ed74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patients with conflicting Outcome or Readmission values:\n",
      "Empty DataFrame\n",
      "Columns: [Outcome, Readmission]\n",
      "Index: []\n",
      "\n",
      "Cumulative distribution (>= records):\n",
      ">= 1 record(s): 33469 patients\n",
      ">= 2 record(s): 32499 patients\n",
      ">= 3 record(s): 26152 patients\n",
      ">= 4 record(s): 20332 patients\n",
      ">= 5 record(s): 15629 patients\n",
      ">= 6 record(s): 12695 patients\n",
      ">= 7 record(s): 10185 patients\n",
      ">= 8 record(s): 8587 patients\n",
      ">= 9 record(s): 7282 patients\n",
      ">= 10 record(s): 6381 patients\n"
     ]
    }
   ],
   "source": [
    "notes = pd.read_csv(NOTES_BACH, dtype={\"PatientID\": \"string\"}, encoding=\"utf-8\", low_memory=False)\n",
    "ehr   = pd.read_csv(EHR_BACH,   dtype={\"PatientID\": \"string\"}, encoding=\"utf-8\", low_memory=False)\n",
    "\n",
    "# Check for patients with conflicting Outcome or Readmission values\n",
    "conflict = ehr.groupby(\"PatientID\")[[\"Outcome\", \"Readmission\"]].nunique()\n",
    "conflict_patients = conflict[(conflict[\"Outcome\"] > 1) | (conflict[\"Readmission\"] > 1)]\n",
    "\n",
    "print(\"Patients with conflicting Outcome or Readmission values:\")\n",
    "print(conflict_patients)\n",
    "\n",
    "dist = (\n",
    "    ehr.groupby(\"PatientID\").size()      # rows per patient\n",
    "       .value_counts()                   # how many patients have that many rows\n",
    "       .sort_index()                     # sort by row count\n",
    ")\n",
    "\n",
    "dist_ge = dist.sort_index(ascending=False).cumsum().sort_index()\n",
    "print(\"\\nCumulative distribution (>= records):\")\n",
    "for rows, n_patients in dist_ge.items():\n",
    "    print(f\">= {rows} record(s): {n_patients} patients\")\n",
    "    if rows >= 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "000dc6d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PatientID</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000</td>\n",
       "      <td>11:30 am chest ( portable ap ) clip # reason :...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10003</td>\n",
       "      <td>admission note d : pt arrived from , sedated o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10004</td>\n",
       "      <td>respiratory care pt was admitted today from os...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10006</td>\n",
       "      <td>full code universal precautions allergy : hepa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10007</td>\n",
       "      <td>12:24 pm chest port . line placement clip # re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  PatientID                                               Text\n",
       "0     10000  11:30 am chest ( portable ap ) clip # reason :...\n",
       "1     10003  admission note d : pt arrived from , sedated o...\n",
       "2     10004  respiratory care pt was admitted today from os...\n",
       "3     10006  full code universal precautions allergy : hepa...\n",
       "4     10007  12:24 pm chest port . line placement clip # re..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine all notes for each patient into a single string, separated by \"\\n\"\n",
    "notes = (\n",
    "    notes\n",
    "    .groupby(\"PatientID\", sort=False)[\"Text\"]\n",
    "    .apply(lambda s: \"\\n\".join(s.astype(str)))\n",
    "    .reset_index(name=\"Text\")\n",
    ")\n",
    "\n",
    "notes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6397e45a",
   "metadata": {},
   "source": [
    "### Method 1: Remove all patients with less than X records, and keep only X records among those qualified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92195890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limited to first 4 EHR rows per patient.\n",
      "Patients removed for having < 4 EHR rows: 13137\n",
      "EHR rows after truncation: 81,328\n",
      "EHR patients: 20,332\n"
     ]
    }
   ],
   "source": [
    "print(f\"Limited to first {RECORDS} EHR rows per patient.\")\n",
    "\n",
    "ehr_counts = ehr.groupby(\"PatientID\").size()\n",
    "valid_ehr_ids = ehr_counts[ehr_counts >= RECORDS].index\n",
    "removed_patients = set(ehr[\"PatientID\"].unique()) - set(valid_ehr_ids)\n",
    "ehr = ehr[ehr[\"PatientID\"].isin(valid_ehr_ids)].reset_index(drop=True)\n",
    "\n",
    "print(f\"Patients removed for having < {RECORDS} EHR rows: {len(removed_patients)}\")\n",
    "\n",
    "ehr = (\n",
    "    ehr\n",
    "    .groupby(\"PatientID\", group_keys=False)\n",
    "    .head(RECORDS)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(f\"EHR rows after truncation: {len(ehr):,}\")\n",
    "print(f\"EHR patients: {ehr['PatientID'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3677adb5",
   "metadata": {},
   "source": [
    "### Method 2: Same as method 1, but we respect the RecordTime (take up to RecordTime X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed66509e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Keep a copy for reporting\n",
    "# _ids_before = set(ehr[\"PatientID\"].unique())\n",
    "# _rows_before = len(ehr)\n",
    "\n",
    "# # Coerce RecordTime to numeric and drop invalid/missing\n",
    "# ehr[\"RecordTime\"] = pd.to_numeric(ehr[\"RecordTime\"], errors=\"coerce\")\n",
    "# ehr = ehr[ehr[\"RecordTime\"].between(1, RECORDS, inclusive=\"both\")].copy()\n",
    "\n",
    "# # Keep only patients that have all RecordTime values 1..RECORDS present\n",
    "# rt_counts = ehr.groupby(\"PatientID\")[\"RecordTime\"].nunique()\n",
    "# valid_ids = rt_counts[rt_counts == RECORDS].index\n",
    "\n",
    "# removed_patients = _ids_before - set(valid_ids)\n",
    "# ehr = ehr[ehr[\"PatientID\"].isin(valid_ids)].sort_values([\"PatientID\", \"RecordTime\"]).reset_index(drop=True)\n",
    "\n",
    "# print(f\"Rows before: {_rows_before:,} | after filtering by RecordTime: {len(ehr):,}\")\n",
    "# print(f\"Patients removed for missing any RecordTime in 1..{RECORDS}: {len(removed_patients):,}\")\n",
    "# print(f\"EHR rows kept: {len(ehr):,}\")\n",
    "# print(f\"EHR patients kept: {ehr['PatientID'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcaa648",
   "metadata": {},
   "source": [
    "### Method 3: Keep all patients, add empty rows for those who don't have enough X records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "feddf63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# id_col   = \"PatientID\"\n",
    "# time_col = \"RecordTime\"\n",
    "# keep_dup = [\"Outcome\", \"Readmission\", \"Sex\", \"Age\"]\n",
    "\n",
    "# print(f\"Targeting exactly {RECORDS} rows per patient (pad with empties; {time_col}=-1).\")\n",
    "\n",
    "# def pad_or_trim(g):\n",
    "#     g = g.sort_values(time_col, kind=\"stable\").head(RECORDS)\n",
    "#     missing = RECORDS - len(g)\n",
    "#     if missing <= 0:\n",
    "#         return g\n",
    "\n",
    "#     # Build a template \"empty\" row\n",
    "#     base = {c: np.nan for c in ehr.columns}\n",
    "#     base[id_col] = g[id_col].iat[0]\n",
    "#     for c in keep_dup:\n",
    "#         if c in g: base[c] = g[c].iat[0]\n",
    "#     # base[time_col] = -1  # negative so it won't look like a latest record\n",
    "#     base[time_col] = 100  # experimental: positive so it will look like a latest record\n",
    "\n",
    "#     filler = pd.DataFrame([base] * missing, columns=ehr.columns)\n",
    "#     return pd.concat([g, filler], ignore_index=True)\n",
    "\n",
    "# ehr = (\n",
    "#     ehr.groupby(id_col, group_keys=False)\n",
    "#        .apply(pad_or_trim)\n",
    "#        .reset_index(drop=True)\n",
    "# )\n",
    "\n",
    "# n_patients = ehr[id_col].nunique()\n",
    "# n_rows     = len(ehr)\n",
    "# n_pad      = int((ehr[time_col] == -1).sum())\n",
    "\n",
    "# print(f\"Patients: {n_patients:,}\")\n",
    "# print(f\"Rows after pad/trim: {n_rows:,} (expected {n_patients*RECORDS:,})\")\n",
    "# print(f\"Padded rows added: {n_pad:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f776e3f",
   "metadata": {},
   "source": [
    "### Keep only patients with both ehr and notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b75076ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BEFORE ===\n",
      "Notes: 31,074 rows | 31,074 unique patients\n",
      "EHR  : 81,328 rows | 20,332 unique patients\n",
      "\n",
      "=== AFTER (kept only patients present in BOTH) ===\n",
      "Common patients kept: 19,307\n",
      "Notes.filtered.csv: 19,307 rows | 19,307 patients\n",
      "ehr.filtered.csv  : 77,228 rows | 19,307 patients\n"
     ]
    }
   ],
   "source": [
    "# normalize IDs a bit (handle stray spaces / empty)\n",
    "notes[\"PatientID\"] = notes[\"PatientID\"].str.strip()\n",
    "ehr[\"PatientID\"]   = ehr[\"PatientID\"].str.strip()\n",
    "\n",
    "# ---- find intersection ----\n",
    "ids_notes = set(notes[\"PatientID\"].unique())\n",
    "ids_ehr   = set(ehr[\"PatientID\"].unique())\n",
    "ids_both  = ids_notes & ids_ehr\n",
    "\n",
    "# ---- filter to the same set ----\n",
    "notes_f = notes[notes[\"PatientID\"].isin(ids_both)].copy()\n",
    "ehr_f   = ehr[ehr[\"PatientID\"].isin(ids_both)].copy()\n",
    "\n",
    "# ---- sort for readability ----\n",
    "notes_f = notes_f.sort_values([\"PatientID\"]).reset_index(drop=True)\n",
    "ehr_f   = ehr_f.sort_values([\"PatientID\", \"RecordTime\"]).reset_index(drop=True)\n",
    "\n",
    "ehr_f.drop(columns=[\"RecordTime\"], inplace=True)\n",
    "\n",
    "# ---- save ----\n",
    "notes_f.to_csv(NOTES_BACH, index=False)\n",
    "ehr_f.to_csv(EHR_BACH,   index=False)\n",
    "\n",
    "# ---- report ----\n",
    "print(\"=== BEFORE ===\")\n",
    "print(f\"Notes: {len(notes):,} rows | {len(ids_notes):,} unique patients\")\n",
    "print(f\"EHR  : {len(ehr):,} rows | {len(ids_ehr):,} unique patients\")\n",
    "print(\"\\n=== AFTER (kept only patients present in BOTH) ===\")\n",
    "print(f\"Common patients kept: {len(ids_both):,}\")\n",
    "print(f\"Notes.filtered.csv: {len(notes_f):,} rows | {notes_f['PatientID'].nunique():,} patients\")\n",
    "print(f\"ehr.filtered.csv  : {len(ehr_f):,} rows | {ehr_f['PatientID'].nunique():,} patients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6e79a7",
   "metadata": {},
   "source": [
    "### Downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc255d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DOWNSAMPLING (REVISED) ===\n",
      "Target: ~15,500 patients with maximized Outcome+ and Readmission+ ratios\n",
      "Current distribution: 19,307 patients\n",
      "Stratum 0 (neither): 16,377\n",
      "Stratum 1 (Readm+ only): 732\n",
      "Stratum 2 (Outcome+ only): 39\n",
      "Stratum 3 (both): 2,159\n",
      "\n",
      "Sampling plan:\n",
      "- Stratum 0 (neither): 12,904\n",
      "- Stratum 1 (Readm+ only): 398\n",
      "- Stratum 2 (Outcome+ only): 39\n",
      "- Stratum 3 (both): 2,159\n",
      "- Total: 15,500\n",
      "- Expected Outcome+: 14.2%\n",
      "- Expected Readm+: 16.5%\n",
      "\n",
      "=== ACTUAL SAMPLING RESULT ===\n",
      "Sampled: 15,500 patients with 14.2% Outcome+, 16.5% Readm+\n"
     ]
    }
   ],
   "source": [
    "# === Downsampling to ~16k patients with maximized outcome/readmission ratios ===\n",
    "print(\"\\n=== DOWNSAMPLING (REVISED) ===\")\n",
    "print(\"Target: ~15,500 patients with maximized Outcome+ and Readmission+ ratios\")\n",
    "\n",
    "# Create patient-level dataframe with one row per patient\n",
    "patient_df = (\n",
    "    ehr_f.groupby(\"PatientID\")\n",
    "    .agg(\n",
    "        Outcome=(\"Outcome\", \"first\"), \n",
    "        Readmission=(\"Readmission\", \"first\")\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Create stratification variable (O*2+R gives us 4 strata)\n",
    "patient_df[\"stratum\"] = (patient_df[\"Outcome\"].astype(int) * 2 + \n",
    "                         patient_df[\"Readmission\"].astype(int))\n",
    "\n",
    "# Current stats\n",
    "stratum_counts = patient_df[\"stratum\"].value_counts().sort_index()\n",
    "total_patients = len(patient_df)\n",
    "\n",
    "print(f\"Current distribution: {total_patients:,} patients\")\n",
    "print(f\"Stratum 0 (neither): {stratum_counts.get(0, 0):,}\")\n",
    "print(f\"Stratum 1 (Readm+ only): {stratum_counts.get(1, 0):,}\")\n",
    "print(f\"Stratum 2 (Outcome+ only): {stratum_counts.get(2, 0):,}\")\n",
    "print(f\"Stratum 3 (both): {stratum_counts.get(3, 0):,}\")\n",
    "\n",
    "# Target total\n",
    "TARGET_SIZE = 15500\n",
    "\n",
    "# Calculate ideal counts to achieve 13% outcome+ and 16.5% readm+\n",
    "ideal_outcome = int(TARGET_SIZE * 0.13)\n",
    "ideal_readm = int(TARGET_SIZE * 0.165)\n",
    "\n",
    "# STEP 1: Take ALL outcome+ patients (strata 2 and 3)\n",
    "take_stratum2 = stratum_counts.get(2, 0)\n",
    "take_stratum3 = stratum_counts.get(3, 0)\n",
    "outcome_patients = take_stratum2 + take_stratum3\n",
    "\n",
    "# STEP 2: Take readm+ patients as needed (from stratum 1)\n",
    "# We already have take_stratum3 patients with readmission+\n",
    "remaining_readm_needed = min(ideal_readm - take_stratum3, stratum_counts.get(1, 0))\n",
    "take_stratum1 = max(0, remaining_readm_needed)\n",
    "\n",
    "# STEP 3: Fill the rest from stratum 0\n",
    "remaining_slots = TARGET_SIZE - take_stratum1 - take_stratum2 - take_stratum3\n",
    "take_stratum0 = min(remaining_slots, stratum_counts.get(0, 0))\n",
    "\n",
    "# Recalculate total size\n",
    "actual_size = take_stratum0 + take_stratum1 + take_stratum2 + take_stratum3\n",
    "\n",
    "# Expected ratios\n",
    "expected_outcome_ratio = (take_stratum2 + take_stratum3) / actual_size\n",
    "expected_readm_ratio = (take_stratum1 + take_stratum3) / actual_size\n",
    "\n",
    "print(f\"\\nSampling plan:\")\n",
    "print(f\"- Stratum 0 (neither): {take_stratum0:,}\")\n",
    "print(f\"- Stratum 1 (Readm+ only): {take_stratum1:,}\")\n",
    "print(f\"- Stratum 2 (Outcome+ only): {take_stratum2:,}\")\n",
    "print(f\"- Stratum 3 (both): {take_stratum3:,}\")\n",
    "print(f\"- Total: {actual_size:,}\")\n",
    "print(f\"- Expected Outcome+: {expected_outcome_ratio:.1%}\")\n",
    "print(f\"- Expected Readm+: {expected_readm_ratio:.1%}\")\n",
    "\n",
    "# Sample patients from each stratum\n",
    "np.random.seed(RANDOM_STATE)\n",
    "sampled_patients = []\n",
    "\n",
    "# Function to sample from a stratum\n",
    "def sample_from_stratum(stratum, count):\n",
    "    if count <= 0:\n",
    "        return []\n",
    "    stratum_patients = patient_df[patient_df[\"stratum\"] == stratum][\"PatientID\"].values\n",
    "    sample_size = min(count, len(stratum_patients))\n",
    "    return np.random.choice(stratum_patients, size=sample_size, replace=False)\n",
    "\n",
    "# Sample from each stratum\n",
    "sampled_patients.extend(sample_from_stratum(3, take_stratum3))\n",
    "sampled_patients.extend(sample_from_stratum(2, take_stratum2))\n",
    "sampled_patients.extend(sample_from_stratum(1, take_stratum1))\n",
    "sampled_patients.extend(sample_from_stratum(0, take_stratum0))\n",
    "\n",
    "# Filter datasets to keep only sampled patients\n",
    "sampled_patient_ids = set(sampled_patients)\n",
    "notes_f = notes_f[notes_f[\"PatientID\"].isin(sampled_patient_ids)].copy()\n",
    "ehr_f = ehr_f[ehr_f[\"PatientID\"].isin(sampled_patient_ids)].copy()\n",
    "\n",
    "# Verify results\n",
    "sampled_pat_outcome = ehr_f.groupby(\"PatientID\")[\"Outcome\"].first()\n",
    "sampled_pat_readm = ehr_f.groupby(\"PatientID\")[\"Readmission\"].first()\n",
    "sampled_outcome_ratio = sampled_pat_outcome.mean()\n",
    "sampled_readm_ratio = sampled_pat_readm.mean()\n",
    "\n",
    "print(\"\\n=== ACTUAL SAMPLING RESULT ===\")\n",
    "print(f\"Sampled: {len(sampled_patient_ids):,} patients with {sampled_outcome_ratio:.1%} Outcome+, {sampled_readm_ratio:.1%} Readm+\")\n",
    "\n",
    "# Save the downsampled data\n",
    "notes_f.to_csv(NOTES_BACH, index=False)\n",
    "ehr_f.to_csv(EHR_BACH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dea3991",
   "metadata": {},
   "source": [
    "### Splitting Train/Test/Val sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c9c22d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cols_between(_df, start_label, end_label=None):\n",
    "    cols = _df.columns\n",
    "    start_idx = cols.get_loc(start_label)\n",
    "    end_idx = len(cols) - 1 if end_label is None else cols.get_loc(end_label)\n",
    "    if start_idx > end_idx:\n",
    "        raise ValueError(f\"{start_label!r} comes after {end_label!r} in columns\")\n",
    "    return cols[start_idx:end_idx + 1]\n",
    "\n",
    "def data_split_and_impute():\n",
    "    df = pd.read_csv(EHR_BACH, encoding='utf-8', low_memory=False)\n",
    "\n",
    "    pat = (\n",
    "        df.groupby('PatientID', as_index=False)\n",
    "            .agg(Outcome=('Outcome','first'),\n",
    "                Readmission=('Readmission','first'))\n",
    "    )\n",
    "    pat['joint'] = pat['Outcome'].astype(int)*2 + pat['Readmission'].astype(int)\n",
    "    print(\"Patient counts per joint class (O*2+R):\", Counter(pat['joint']))\n",
    "\n",
    "    # 1) TEST = 20% (stratified on joint)\n",
    "    pat_trainval, pat_test = train_test_split(\n",
    "        pat, test_size=0.20, stratify=pat['joint'], random_state=RANDOM_STATE\n",
    "    )\n",
    "\n",
    "    # 2) VAL = 12.5% of remaining (i.e., ~10% overall)\n",
    "    pat_train, pat_val = train_test_split(\n",
    "        pat_trainval, test_size=0.125, stratify=pat_trainval['joint'], random_state=RANDOM_STATE\n",
    "    )\n",
    "\n",
    "    print(\"Train/Val/Test patients:\", len(pat_train), len(pat_val), len(pat_test))\n",
    "\n",
    "    train_ids = set(pat_train['PatientID'])\n",
    "    val_ids   = set(pat_val['PatientID'])\n",
    "    test_ids  = set(pat_test['PatientID'])\n",
    "\n",
    "    train_df = df[df['PatientID'].isin(train_ids)].copy()\n",
    "    val_df   = df[df['PatientID'].isin(val_ids)].copy()\n",
    "    test_df  = df[df['PatientID'].isin(test_ids)].copy()\n",
    "\n",
    "    for name, d in [('Train', train_df), ('Val', val_df), ('Test', test_df)]:\n",
    "        print(f\"{name}: rows={len(d):,}, patients={d['PatientID'].nunique():,}\")\n",
    "\n",
    "    # ====== DEFINE FEATURE BLOCKS ======\n",
    "    cat_cols = list(cols_between(\n",
    "        df,\n",
    "        \"Capillary refill rate->0.0\",\n",
    "        \"Glascow coma scale verbal response->3 Inapprop words\"\n",
    "    ))\n",
    "    num_cols = list(cols_between(df, \"Diastolic blood pressure\", None))\n",
    "\n",
    "    # ====== COMPUTE IMPUTATION VALUES ON TRAIN ONLY ======\n",
    "    # Categorical: all NaNs -> 0 (no stats needed)\n",
    "    # Numeric: per-column mean from TRAIN\n",
    "    # Ensure numeric dtype for means; if some numeric cols are object due to bad parsing, coerce safely\n",
    "    train_num = train_df[num_cols].apply(pd.to_numeric, errors='coerce')\n",
    "    num_impute = train_num.mean()  # pandas Series indexed by column name\n",
    "\n",
    "    # ====== APPLY IMPUTATION (USING TRAIN STATS) ======\n",
    "    def apply_impute(d):\n",
    "        d = d.copy()\n",
    "        if cat_cols:\n",
    "            d.loc[:, cat_cols] = d.loc[:, cat_cols].fillna(0)\n",
    "        if num_cols:\n",
    "            d.loc[:, num_cols] = d.loc[:, num_cols].apply(pd.to_numeric, errors='coerce')\n",
    "            # d.loc[:, num_cols] = d.loc[:, num_cols].fillna(num_impute)\n",
    "            d.loc[:, num_cols] = d.loc[:, num_cols].fillna(0)  # experimental: simpler\n",
    "        return d\n",
    "\n",
    "    train_df_i = apply_impute(train_df)\n",
    "    val_df_i   = apply_impute(val_df)\n",
    "    test_df_i  = apply_impute(test_df)\n",
    "\n",
    "    # ====== QUICK CHECKS AFTER IMPUTATION ======\n",
    "    def summarize_split(name, df_rows, df_pat):\n",
    "        o_row = df_rows['Outcome'].mean()\n",
    "        r_row = df_rows['Readmission'].mean()\n",
    "        o_pat = df_pat['Outcome'].mean()\n",
    "        r_pat = df_pat['Readmission'].mean()\n",
    "        print(\n",
    "            f\"{name} ‚Äî Outcome: rows={o_row:.3%}, patients={o_pat:.3%} | \"\n",
    "            f\"Readmission: rows={r_row:.3%}, patients={r_pat:.3%}\"\n",
    "            f\"NaN: {(df_rows.isna().sum().sum())}\"\n",
    "        )\n",
    "\n",
    "    summarize_split(\"Train\", train_df_i, pat_train)\n",
    "    summarize_split(\"Val\",   val_df_i,   pat_val)\n",
    "    summarize_split(\"Test\",  test_df_i,  pat_test)\n",
    "\n",
    "    train_df_i.to_csv(TRAIN_DRAFT, index=False)\n",
    "    val_df_i.to_csv(VAL_DRAFT, index=False)\n",
    "    test_df_i.to_csv(TEST_DRAFT, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ccd5076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient counts per joint class (O*2+R): Counter({0: 12904, 3: 2159, 1: 398, 2: 39})\n",
      "Train/Val/Test patients: 10850 1550 3100\n",
      "Train: rows=43,400, patients=10,850\n",
      "Val: rows=6,200, patients=1,550\n",
      "Test: rows=12,400, patients=3,100\n",
      "Train ‚Äî Outcome: rows=14.175%, patients=14.175% | Readmission: rows=16.498%, patients=16.498%NaN: 0\n",
      "Val ‚Äî Outcome: rows=14.194%, patients=14.194% | Readmission: rows=16.516%, patients=16.516%NaN: 0\n",
      "Test ‚Äî Outcome: rows=14.194%, patients=14.194% | Readmission: rows=16.484%, patients=16.484%NaN: 0\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(TRAIN_DRAFT) or not os.path.exists(VAL_DRAFT) or not os.path.exists(TEST_DRAFT):\n",
    "    data_split_and_impute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1177da",
   "metadata": {},
   "source": [
    "## II. Check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf9c213",
   "metadata": {},
   "source": [
    "### Check Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cecaad6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üåç Overall stats:\n",
      "{'max_chars': 52570, 'avg_chars': np.float64(9449.687483870968), 'max_words': 9521, 'avg_words': np.float64(1795.2187741935484)}\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(NOTES_BACH, encoding=\"utf-8\", low_memory=False)\n",
    "\n",
    "def char_count(text):\n",
    "    return len(text)\n",
    "def word_count(text):\n",
    "    return len(text.split())\n",
    "\n",
    "df[\"char_count\"] = df[\"Text\"].astype(str).apply(char_count)\n",
    "df[\"word_count\"] = df[\"Text\"].astype(str).apply(word_count)\n",
    "\n",
    "overall = {\n",
    "    \"max_chars\": df[\"char_count\"].max(),\n",
    "    \"avg_chars\": df[\"char_count\"].mean(),\n",
    "    \"max_words\": df[\"word_count\"].max(),\n",
    "    \"avg_words\": df[\"word_count\"].mean(),\n",
    "}\n",
    "\n",
    "print(\"\\nüåç Overall stats:\")\n",
    "print(overall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317e924d",
   "metadata": {},
   "source": [
    "### Check EHR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb62e9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (62000, 64)\n",
      "Outcome positive ratio (each episode): 0.14180645161290323\n",
      "Readmission positive ratio (each episode): 0.16496774193548386\n",
      "===\n",
      "Outcome positive ratio (each patient): 0.14180645161290323\n",
      "Readmission positive ratio (each patient): 0.16496774193548386\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(EHR_BACH, encoding=\"utf-8\", low_memory=False)\n",
    "print(\"Shape:\", df.shape)\n",
    "\n",
    "print(\"Outcome positive ratio (each episode):\", df[\"Outcome\"].mean())\n",
    "print(\"Readmission positive ratio (each episode):\", df[\"Readmission\"].mean())\n",
    "\n",
    "print(\"===\")\n",
    "pat_any = df.groupby(\"PatientID\")[[\"Outcome\",\"Readmission\"]].max()\n",
    "\n",
    "print(\"Outcome positive ratio (each patient):\", pat_any[\"Outcome\"].mean())\n",
    "print(\"Readmission positive ratio (each patient):\", pat_any[\"Readmission\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a38ff03",
   "metadata": {},
   "source": [
    "# Part 2: Preprocess PrimeKG and Extract Entities from Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c50cb7",
   "metadata": {},
   "source": [
    "## PrimeKG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef3ac589",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 30 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = BGEM3FlagModel(\"BAAI/bge-m3\", use_fp16=True, device=device, trust_remote_code=True)\n",
    "\n",
    "def create_adj():\n",
    "    # Create adjacency list\n",
    "    df = pd.read_csv(PRIMEKG_KG, low_memory=False, encoding=\"utf-8\")\n",
    "    df = df[[\"relation\", \"x_index\", \"y_index\"]]\n",
    "\n",
    "    adj_list = defaultdict(list)\n",
    "    for u, v, r in tqdm(zip(df[\"x_index\"].values, df[\"y_index\"].values, df[\"relation\"].values), total=len(df), desc=\"Creating adjacency list\"):\n",
    "        adj_list[int(u)].append((int(v), str(r)))\n",
    "\n",
    "    with open(KG_ADJACENCY, \"wb\") as f:\n",
    "        pickle.dump(adj_list, f, protocol=5)\n",
    "\n",
    "def create_disease_features():\n",
    "    # Create disease feature\n",
    "    df = pd.read_csv(PRIMEKG_DISEASE, low_memory=False, encoding=\"utf-8\")\n",
    "    df = df.sort_values(\"node_index\").reset_index(drop=True)\n",
    "\n",
    "    df[\"Diseases\"] = (\n",
    "        \"[disease name]\" + df[\"mondo_name\"].fillna(\"\") + \" \" +\n",
    "        \"[definition]\" + df[\"mondo_definition\"].combine_first(df[\"orphanet_definition\"]).fillna(\"\") + \" \" +\n",
    "        \"[description]\" + df[\"umls_description\"].fillna(\"\")\n",
    "    )\n",
    "    df[\"embed\"] = list(batch_encode(df[\"Diseases\"].tolist(), batch_size=64, max_length=8192).cpu().numpy())\n",
    "    df = df[[\"node_index\", \"mondo_name\", \"Diseases\", \"embed\"]]\n",
    "\n",
    "    with open(DISEASE_FEATURES, \"wb\") as f:\n",
    "        pickle.dump(df, f, protocol=5)\n",
    "\n",
    "def create_notes_embeddings():\n",
    "    # Embed notes and save to HDF5\n",
    "    with h5py.File(NOTES_EMBEDDINGS, \"w\") as f: # reset file\n",
    "        pass\n",
    "\n",
    "    notes_df = pd.read_csv(NOTES_BACH, encoding=\"utf-8\", low_memory=False)\n",
    "    for idx, row in tqdm(notes_df.iterrows(), total=len(notes_df), desc=\"Embedding notes and saving to HDF5\"):\n",
    "        patient_id = row[\"PatientID\"]\n",
    "        text = row[\"Text\"]\n",
    "        \n",
    "        with h5py.File(NOTES_EMBEDDINGS, \"a\") as h5:\n",
    "            grp = h5.create_group(str(patient_id))\n",
    "            grp.create_dataset(\"PatientID\", data=np.asarray(patient_id, dtype=\"int64\"))\n",
    "            # grp.create_dataset(\"Note\", data=langchain_chunk_embed(text), compression=\"gzip\")\n",
    "            grp.create_dataset(\"Note\", data=plain_truncate(text=text, max_length=256), compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ec2b402",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding notes and saving to HDF5:   0%|          | 0/15500 [00:00<?, ?it/s]Input ids are automatically padded to be a multiple of `config.attention_window`: 512\n",
      "Embedding notes and saving to HDF5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15500/15500 [21:05<00:00, 12.25it/s] \n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(KG_ADJACENCY):\n",
    "    create_adj()\n",
    "\n",
    "if not os.path.exists(DISEASE_FEATURES):\n",
    "    create_disease_features()\n",
    "\n",
    "if not os.path.exists(NOTES_EMBEDDINGS):\n",
    "    create_notes_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8afd3f5",
   "metadata": {},
   "source": [
    "## Extract Entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991cf544",
   "metadata": {},
   "source": [
    "### Create Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0f891df",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DISEASE_FEATURES, \"rb\") as f:\n",
    "    kg = pickle.load(f)\n",
    "mapping = dict(zip(kg[\"node_index\"], kg[\"mondo_name\"]))\n",
    "\n",
    "with open(KG_ADJACENCY, \"rb\") as f:\n",
    "    adj = pickle.load(f)\n",
    "\n",
    "df = pd.read_csv(NOTES_BACH, encoding=\"utf-8\", low_memory=False)\n",
    "notes_text = {}\n",
    "for _, row in df.iterrows():\n",
    "    pid = row[\"PatientID\"]\n",
    "    text = row[\"Text\"]\n",
    "    notes_text[pid] = text\n",
    "\n",
    "notes_emb = {}\n",
    "with h5py.File(NOTES_EMBEDDINGS, \"r\") as h5:\n",
    "    for patient_id in h5.keys():  # each group is named by patient_id\n",
    "        grp = h5[patient_id]\n",
    "        pid = int(grp[\"PatientID\"][()])\n",
    "        embedding = np.array(grp[\"Note\"])\n",
    "        notes_emb[pid] = embedding\n",
    "# print(len(notes_emb.keys()))\n",
    "# print(notes_emb.keys())\n",
    "# print(notes_emb[100].shape) # 768\n",
    "\n",
    "def _to_float32_array(x):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x.detach().cpu().float().numpy()\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return x.astype(\"float32\", copy=False)\n",
    "    raise TypeError(f\"Expected tensor/ndarray, got {type(x)}\")\n",
    "\n",
    "def store_patient(h5_path, p, ehr, target, notes, summary):\n",
    "    with h5py.File(h5_path, \"a\") as h5:\n",
    "        grp = h5.create_group(str(p))\n",
    "        grp.create_dataset(\"PatientID\", data=np.asarray(p, dtype=\"int64\"))\n",
    "        grp.create_dataset(\"X\", data=ehr, compression=\"gzip\")\n",
    "        grp.create_dataset(\"Note\", data=_to_float32_array(notes), compression=\"gzip\")\n",
    "        grp.create_dataset(\"Summary\", data=_to_float32_array(summary), compression=\"gzip\")\n",
    "        grp.create_dataset(\"Y\", data=np.asarray(target, dtype=\"int8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de6f0b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORKERS = 1\n",
    "\n",
    "def create_summary_embeddings(mode = \"train\"):\n",
    "    if mode not in [\"train\", \"val\", \"test\"]:\n",
    "        raise ValueError(\"mode must be 'train', 'val', or 'test'\")\n",
    "    if mode == \"train\":\n",
    "        df = pd.read_csv(TRAIN_DRAFT, encoding=\"utf-8\", low_memory=False)\n",
    "        h5_path = SUMEMB_TRAIN\n",
    "    elif mode == \"val\":\n",
    "        df = pd.read_csv(VAL_DRAFT, encoding=\"utf-8\", low_memory=False)\n",
    "        h5_path = SUMEMB_VAL\n",
    "    else:\n",
    "        df = pd.read_csv(TEST_DRAFT, encoding=\"utf-8\", low_memory=False)\n",
    "        h5_path = SUMEMB_TEST\n",
    "\n",
    "    with h5py.File(h5_path, \"w\") as f:\n",
    "        pass\n",
    "    \n",
    "    cat_cols = list(cols_between(\n",
    "        df,\n",
    "        \"Capillary refill rate->0.0\",\n",
    "        \"Glascow coma scale verbal response->3 Inapprop words\"\n",
    "    ))\n",
    "    num_cols = list(cols_between(df, \"Diastolic blood pressure\", None))\n",
    "\n",
    "    col_mean = df[num_cols].mean()\n",
    "    col_std = df[num_cols].std()\n",
    "\n",
    "    entities = defaultdict(list)\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=f\"Processing {mode} data\"):\n",
    "        record = \"\"\n",
    "        PatientID = row[\"PatientID\"]\n",
    "\n",
    "        if row[\"Sex\"] == 1:\n",
    "            record += \"Gender: Male\\n\"\n",
    "        else:\n",
    "            record += \"Gender: Female\\n\"\n",
    "        record += f\"Age: {row['Age']}\\n\"\n",
    "        \n",
    "        for c in cat_cols:\n",
    "            if row[c] == 1:\n",
    "                cat = c\n",
    "                if \"Glascow coma scale total\" not in cat:\n",
    "                    for i in range(0, 30, 1):\n",
    "                        cat = cat.replace(f\"->{i}.0\", \" : \")\n",
    "                        cat = cat.replace(f\"->{i}\", \" : \")\n",
    "                cat = cat.replace(\"->\", \" : \")\n",
    "                entities[PatientID].append(cat)\n",
    "        \n",
    "        for c in num_cols:\n",
    "            if math.isnan(row[c]):\n",
    "                continue\n",
    "            z_score = (row[c] - col_mean[c]) / col_std[c]\n",
    "            if z_score > 2:\n",
    "                entities[PatientID].append(f\"{c} too high\")\n",
    "            elif z_score < -2:\n",
    "                entities[PatientID].append(f\"{c} too low\")\n",
    "\n",
    "    # Match entities to knowledge graph\n",
    "    patients = list(df[\"PatientID\"].unique())\n",
    "\n",
    "    def get_summary(p):\n",
    "        entities[p] = list(set(entities[p]))\n",
    "        summary_entities = \"\"\n",
    "        summary_nodes = \"\"\n",
    "        summary_edges = \"\"\n",
    "        nodes = []\n",
    "        for e in entities[p]:\n",
    "            summary_entities += e + \", \"\n",
    "            idx = cosine_filter(None, e, threshold=0.6, top_k=3)\n",
    "            nodes.extend(idx)\n",
    "        summary_entities = summary_entities[:-2]\n",
    "\n",
    "        nodes = list(set(nodes))\n",
    "        \n",
    "        for n in nodes:\n",
    "            summary_nodes += kg.iloc[n][\"Diseases\"] + \", \"\n",
    "            node_x = kg.iloc[n][\"node_index\"]\n",
    "            for connect_to in adj[n]:\n",
    "                rela = connect_to[1]\n",
    "                node_y = connect_to[0]\n",
    "                if node_y not in kg[\"node_index\"].values:\n",
    "                    continue\n",
    "                e = \"(\" + mapping[node_x] + \", \" + str(rela) + \", \" + mapping[node_y] + \")\"\n",
    "                # print(e)\n",
    "                summary_edges += e + \", \"\n",
    "\n",
    "        summary_edges = summary_edges[:-2]\n",
    "        summary_nodes = summary_nodes[:-2]\n",
    "        summary_notes = extract_note(notes=notes_text[p])\n",
    "\n",
    "        return create_summary(notes=summary_notes,\n",
    "                              ehr=summary_entities,\n",
    "                              nodes=summary_nodes,\n",
    "                              edges=summary_edges,\n",
    "                              )\n",
    "\n",
    "    def worker_summary_only(patient_id):\n",
    "        return patient_id, get_summary(patient_id)\n",
    "\n",
    "    summaries = {}\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "        futures = {ex.submit(worker_summary_only, pid): pid for pid in patients}\n",
    "        for fut in tqdm(as_completed(futures), total=len(futures), desc=\"Summaries embedding...\", unit=\"pt\"):\n",
    "            pid, summary = fut.result()\n",
    "            summaries[pid] = summary\n",
    "\n",
    "    text_dtype = h5py.string_dtype(encoding=\"utf-8\")\n",
    "    with h5py.File(h5_path, \"a\") as h5:\n",
    "        for pid, summary in tqdm(summaries.items(), desc=\"Writing HDF5\", unit=\"pt\"):\n",
    "            gname = str(pid)\n",
    "            if gname in h5:\n",
    "                del h5[gname]\n",
    "            grp = h5.create_group(gname)\n",
    "            grp.create_dataset(\"PatientID\", data=np.asarray(pid, dtype=\"int64\"))\n",
    "            grp.create_dataset(\"SummaryText\", data=np.asarray(summary, dtype=object), dtype=text_dtype)\n",
    "            grp.create_dataset(\"SummaryEmbedding\", data=langchain_chunk_embed(summary), compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8462f372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not os.path.exists(SUMEMB_TRAIN):\n",
    "#     create_summary_embeddings(\"train\")\n",
    "\n",
    "# if not os.path.exists(SUMEMB_VAL):\n",
    "#     create_summary_embeddings(\"val\")\n",
    "\n",
    "# if not os.path.exists(SUMEMB_TEST):\n",
    "#     create_summary_embeddings(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08db9c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = {}\n",
    "def load_summary_embeddings(h5_path):\n",
    "    with h5py.File(h5_path, \"r\") as h5:\n",
    "        for gname, grp in h5.items():\n",
    "            summaries[int(gname)] = {\n",
    "                \"PatientID\": int(grp[\"PatientID\"][()]),\n",
    "                \"SummaryText\": grp[\"SummaryText\"].asstr()[()],\n",
    "                \"SummaryEmbedding\": grp[\"SummaryEmbedding\"][()]\n",
    "            }\n",
    "\n",
    "def create_dataset(mode = \"train\"):\n",
    "    if mode not in [\"train\", \"val\", \"test\"]:\n",
    "        raise ValueError(\"mode must be 'train', 'val', or 'test'\")\n",
    "    if mode == \"train\":\n",
    "        df = pd.read_csv(TRAIN_DRAFT, encoding=\"utf-8\", low_memory=False)\n",
    "        h5_path = TRAIN\n",
    "        # load_summary_embeddings(SUMEMB_TRAIN)\n",
    "    elif mode == \"val\":\n",
    "        df = pd.read_csv(VAL_DRAFT, encoding=\"utf-8\", low_memory=False)\n",
    "        h5_path = VAL\n",
    "        # load_summary_embeddings(SUMEMB_VAL)\n",
    "    else:\n",
    "        df = pd.read_csv(TEST_DRAFT, encoding=\"utf-8\", low_memory=False)\n",
    "        h5_path = TEST\n",
    "        # load_summary_embeddings(SUMEMB_TEST)\n",
    "\n",
    "    with h5py.File(h5_path, \"w\") as f:\n",
    "        pass\n",
    "\n",
    "    patients = list(df[\"PatientID\"].unique())\n",
    "    feature_cols = [c for c in df.columns if c not in [\"PatientID\",\"Outcome\",\"Readmission\"]]\n",
    "    target_map = df.groupby(\"PatientID\")[[\"Outcome\",\"Readmission\"]].first()\n",
    "\n",
    "    for p in tqdm(patients, total=len(patients), desc=f\"Storing {mode} data to HDF5\"):\n",
    "        data_ehr = df.loc[df[\"PatientID\"] == p, feature_cols].to_numpy()\n",
    "        data_notes = notes_emb[p]\n",
    "        data_summary = data_notes # fallback to notes embedding if summary missing\n",
    "        # data_summary = summaries[p][\"SummaryEmbedding\"]\n",
    "        outcome, readm = target_map.loc[p].astype(int)\n",
    "        data_target = (int(outcome), int(readm))\n",
    "        store_patient(h5_path, p, data_ehr, data_target, data_notes, data_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fdd1a876",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Storing train data to HDF5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10850/10850 [08:02<00:00, 22.48it/s]\n",
      "Storing val data to HDF5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1550/1550 [00:58<00:00, 26.45it/s]\n",
      "Storing test data to HDF5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3100/3100 [01:44<00:00, 29.80it/s]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(TRAIN) or not os.path.exists(VAL) or not os.path.exists(TEST):\n",
    "    create_dataset(mode=\"train\")\n",
    "    create_dataset(mode=\"val\")\n",
    "    create_dataset(mode=\"test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
