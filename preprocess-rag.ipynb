{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f2bda56",
   "metadata": {},
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17919e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "#\n",
    "from utils.bgem3 import cosine_filter, batch_encode\n",
    "from utils.call_llm import extract_note, create_summary\n",
    "from utils.clinical_longformer import langchain_chunk_embed, plain_truncate\n",
    "from utils.constants import *\n",
    "#\n",
    "import torch\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "#\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict, Counter\n",
    "import pickle\n",
    "import h5py\n",
    "#\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a38ff03",
   "metadata": {},
   "source": [
    "# Part 3: Entity Extraction and Create Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f891df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cols_between(_df, start_label, end_label=None):\n",
    "    cols = _df.columns\n",
    "    start_idx = cols.get_loc(start_label)\n",
    "    end_idx = len(cols) - 1 if end_label is None else cols.get_loc(end_label)\n",
    "    if start_idx > end_idx:\n",
    "        raise ValueError(f\"{start_label!r} comes after {end_label!r} in columns\")\n",
    "    return cols[start_idx:end_idx + 1]\n",
    "\n",
    "with open(DISEASE_FEATURES, \"rb\") as f:\n",
    "    kg = pickle.load(f)\n",
    "mapping_disease = dict(zip(kg[\"node_index\"], kg[\"mondo_name\"]))\n",
    "\n",
    "with open(KG_ADJACENCY, \"rb\") as f:\n",
    "    adj = pickle.load(f)\n",
    "\n",
    "with open(NOTES_EXTRACTS, \"rb\") as f:\n",
    "    extracts = pickle.load(f)\n",
    "notes_extract = dict(zip(extracts[\"PatientID\"], extracts[\"Extract\"]))\n",
    "\n",
    "notes_emb = {}\n",
    "with h5py.File(NOTES_EMBEDDINGS, \"r\") as h5:\n",
    "    for patient_id in h5.keys():  # each group is named by patient_id\n",
    "        grp = h5[patient_id]\n",
    "        pid = int(grp[\"PatientID\"][()])\n",
    "        notes_emb[pid] = np.array(grp[\"Note\"])\n",
    "# print(len(notes_emb.keys()))\n",
    "# print(notes_emb[100].shape) # 768\n",
    "\n",
    "def _to_float32_array(x):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x.detach().cpu().float().numpy()\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return x.astype(\"float32\", copy=False)\n",
    "    raise TypeError(f\"Expected tensor/ndarray, got {type(x)}\")\n",
    "\n",
    "def store_patient(h5_path, p, ehr, target, notes, summary):\n",
    "    with h5py.File(h5_path, \"a\") as h5:\n",
    "        grp = h5.create_group(str(p))\n",
    "        grp.create_dataset(\"PatientID\", data=np.asarray(p, dtype=\"int64\"))\n",
    "        grp.create_dataset(\"X\", data=ehr, compression=\"gzip\")\n",
    "        grp.create_dataset(\"Note\", data=_to_float32_array(notes), compression=\"gzip\")\n",
    "        grp.create_dataset(\"Summary\", data=_to_float32_array(summary), compression=\"gzip\")\n",
    "        grp.create_dataset(\"Y\", data=np.asarray(target, dtype=\"int8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b5dc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary_embeddings(mode = \"train\"):\n",
    "    if mode not in [\"train\", \"val\", \"test\"]:\n",
    "        raise ValueError(\"mode must be 'train', 'val', or 'test'\")\n",
    "    if mode == \"train\":\n",
    "        df = pd.read_csv(TRAIN_DRAFT, encoding=\"utf-8\", low_memory=False)\n",
    "        h5_path = SUMEMB_TRAIN\n",
    "    elif mode == \"val\":\n",
    "        df = pd.read_csv(VAL_DRAFT, encoding=\"utf-8\", low_memory=False)\n",
    "        h5_path = SUMEMB_VAL\n",
    "    else:\n",
    "        df = pd.read_csv(TEST_DRAFT, encoding=\"utf-8\", low_memory=False)\n",
    "        h5_path = SUMEMB_TEST\n",
    "\n",
    "    with h5py.File(h5_path, \"w\") as f:\n",
    "        pass\n",
    "    \n",
    "    cat_cols = list(cols_between(\n",
    "        df,\n",
    "        \"Capillary refill rate->0.0\",\n",
    "        \"Glascow coma scale verbal response->3 Inapprop words\"\n",
    "    ))\n",
    "    num_cols = list(cols_between(df, \"Diastolic blood pressure\", None))\n",
    "\n",
    "    patients = list(df[\"PatientID\"].unique())\n",
    "    col_mean = df[num_cols].mean()\n",
    "    col_std = df[num_cols].std()\n",
    "\n",
    "    preprocess_patients = {}\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=f\"Processing {mode} data\"):\n",
    "        PatientID = row[\"PatientID\"]\n",
    "        entities = []\n",
    "\n",
    "        # record = \"\"\n",
    "        # if row[\"Sex\"] == 1:\n",
    "        #     record += \"Gender: Male\\n\"\n",
    "        # else:\n",
    "        #     record += \"Gender: Female\\n\"\n",
    "        # record += f\"Age: {row['Age']}\\n\"\n",
    "        \n",
    "        for c in cat_cols:\n",
    "            if row[c] == 1:\n",
    "                cat = c\n",
    "                if \"Glascow coma scale total\" not in cat:\n",
    "                    for i in range(0, 30, 1):\n",
    "                        cat = cat.replace(f\"->{i}.0\", \" : \")\n",
    "                        cat = cat.replace(f\"->{i}\", \" : \")\n",
    "                cat = cat.replace(\"->\", \" : \")\n",
    "                entities.append(cat)\n",
    "        \n",
    "        for c in num_cols:\n",
    "            if math.isnan(row[c]):\n",
    "                continue\n",
    "            z_score = (row[c] - col_mean[c]) / col_std[c]\n",
    "            if z_score > 2:\n",
    "                entities.append(f\"{c} too high\")\n",
    "            elif z_score < -2:\n",
    "                entities.append(f\"{c} too low\")\n",
    "\n",
    "        entities = list(set(entities))\n",
    "        summary_entities = \"\"\n",
    "        summary_nodes = \"\"\n",
    "        summary_edges = \"\"\n",
    "        nodes = []\n",
    "        for e in entities:\n",
    "            summary_entities += e + \", \"\n",
    "            idx = cosine_filter(None, e, threshold=0.6, top_k=3)\n",
    "            nodes.extend(idx)\n",
    "        summary_entities = summary_entities[:-2]\n",
    "\n",
    "        nodes = list(set(nodes))\n",
    "        \n",
    "        for n in nodes:\n",
    "            summary_nodes += kg.iloc[n][\"Diseases\"] + \", \"\n",
    "            node_x = kg.iloc[n][\"node_index\"]\n",
    "            for connect_to in adj[n]:\n",
    "                rela = connect_to[1]\n",
    "                node_y = connect_to[0]\n",
    "                if node_y not in kg[\"node_index\"].values:\n",
    "                    continue\n",
    "                e = \"(\" + mapping_disease[node_x] + \", \" + str(rela) + \", \" + mapping_disease[node_y] + \")\"\n",
    "                # print(e)\n",
    "                summary_edges += e + \", \"\n",
    "\n",
    "        summary_edges = summary_edges[:-2]\n",
    "        summary_nodes = summary_nodes[:-2]\n",
    "        summary_notes = notes_extract.get(PatientID, \"\")\n",
    "\n",
    "        preprocess_patients[PatientID] = {\n",
    "            \"notes\": summary_notes,\n",
    "            \"ehr\": summary_entities,\n",
    "            \"nodes\": summary_nodes,\n",
    "            \"edges\": summary_edges,\n",
    "        }\n",
    "    \n",
    "    def get_summary(p):\n",
    "        return create_summary(notes=preprocess_patients[p][\"notes\"],\n",
    "                              ehr=preprocess_patients[p][\"ehr\"],\n",
    "                              nodes=preprocess_patients[p][\"nodes\"],\n",
    "                              edges=preprocess_patients[p][\"edges\"],\n",
    "                              )\n",
    "\n",
    "    summaries = {}\n",
    "    def worker_summary(pid):\n",
    "        return pid, get_summary(pid)\n",
    "    progress_bar = tqdm(total=len(patients), desc=\"Generating summaries\", unit=\"pt\")\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        future_to_pid = {executor.submit(worker_summary, pid): pid for pid in patients}\n",
    "        for future in as_completed(future_to_pid):\n",
    "            pid, summary = future.result()\n",
    "            if summary is not None:\n",
    "                summaries[pid] = summary\n",
    "            progress_bar.update(1)\n",
    "    progress_bar.close()\n",
    "\n",
    "    text_dtype = h5py.string_dtype(encoding=\"utf-8\")\n",
    "    with h5py.File(h5_path, \"a\") as h5:\n",
    "        for pid, summary in tqdm(summaries.items(), desc=\"Writing HDF5\", unit=\"pt\"):\n",
    "            gname = str(pid)\n",
    "            if gname in h5:\n",
    "                del h5[gname]\n",
    "            grp = h5.create_group(gname)\n",
    "            grp.create_dataset(\"PatientID\", data=np.asarray(pid, dtype=\"int64\"))\n",
    "            grp.create_dataset(\"SummaryText\", data=np.asarray(summary, dtype=object), dtype=text_dtype)\n",
    "            grp.create_dataset(\"SummaryEmbedding\", data=langchain_chunk_embed(summary) if USE_CHUNKING else plain_truncate(text=summary, max_length=256), compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6f0b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary_embeddings(mode = \"train\"):\n",
    "    if mode not in [\"train\", \"val\", \"test\"]:\n",
    "        raise ValueError(\"mode must be 'train', 'val', or 'test'\")\n",
    "    if mode == \"train\":\n",
    "        df = pd.read_csv(TRAIN_DRAFT, encoding=\"utf-8\", low_memory=False)\n",
    "        h5_path = SUMEMB_TRAIN\n",
    "    elif mode == \"val\":\n",
    "        df = pd.read_csv(VAL_DRAFT, encoding=\"utf-8\", low_memory=False)\n",
    "        h5_path = SUMEMB_VAL\n",
    "    else:\n",
    "        df = pd.read_csv(TEST_DRAFT, encoding=\"utf-8\", low_memory=False)\n",
    "        h5_path = SUMEMB_TEST\n",
    "\n",
    "    with h5py.File(h5_path, \"w\") as f:\n",
    "        pass\n",
    "    \n",
    "    cat_cols = list(cols_between(\n",
    "        df,\n",
    "        \"Capillary refill rate->0.0\",\n",
    "        \"Glascow coma scale verbal response->3 Inapprop words\"\n",
    "    ))\n",
    "    num_cols = list(cols_between(df, \"Diastolic blood pressure\", None))\n",
    "\n",
    "    patients = list(df[\"PatientID\"].unique())\n",
    "    col_mean = df[num_cols].mean()\n",
    "    col_std = df[num_cols].std()\n",
    "\n",
    "    # Define a worker function to process a patient's data\n",
    "    def process_patient(patient_id):\n",
    "        patient_entities = []\n",
    "        patient_rows = df[df[\"PatientID\"] == patient_id]\n",
    "        \n",
    "        for _, row in patient_rows.iterrows():\n",
    "            # Process categorical columns\n",
    "            for c in cat_cols:\n",
    "                if row[c] == 1:\n",
    "                    cat = c\n",
    "                    if \"Glascow coma scale total\" not in cat:\n",
    "                        for i in range(0, 30, 1):\n",
    "                            cat = cat.replace(f\"->{i}.0\", \" : \")\n",
    "                            cat = cat.replace(f\"->{i}\", \" : \")\n",
    "                    cat = cat.replace(\"->\", \" : \")\n",
    "                    patient_entities.append(cat)\n",
    "            \n",
    "            # Process numerical columns\n",
    "            for c in num_cols:\n",
    "                if math.isnan(row[c]):\n",
    "                    continue\n",
    "                z_score = (row[c] - col_mean[c]) / col_std[c]\n",
    "                if z_score > 2:\n",
    "                    patient_entities.append(f\"{c} too high\")\n",
    "                elif z_score < -2:\n",
    "                    patient_entities.append(f\"{c} too low\")\n",
    "        \n",
    "        # Remove duplicates\n",
    "        patient_entities = list(set(patient_entities))\n",
    "        \n",
    "        # Create summary strings and find nodes\n",
    "        nodes = []\n",
    "        summary_entity = \"\"\n",
    "        for e in patient_entities:\n",
    "            summary_entity += e + \", \"\n",
    "            idx = cosine_filter(query=e, threshold=0.6, top_k=3)\n",
    "            nodes.extend(idx)\n",
    "        summary_entity = summary_entity[:-2] if summary_entity else \"\"\n",
    "        \n",
    "        # Remove duplicate nodes\n",
    "        nodes = list(set(nodes))\n",
    "        \n",
    "        # Process nodes and edges\n",
    "        summary_node = \"\"\n",
    "        summary_edge = \"\"\n",
    "        for n in nodes:\n",
    "            summary_node += kg.iloc[n][\"Diseases\"] + \", \"\n",
    "            node_x = kg.iloc[n][\"node_index\"]\n",
    "            for connect_to in adj[n]:\n",
    "                rela = connect_to[1]\n",
    "                node_y = connect_to[0]\n",
    "                if node_y not in kg[\"node_index\"].values:\n",
    "                    continue\n",
    "                e = \"(\" + mapping_disease[node_x] + \", \" + str(rela) + \", \" + mapping_disease[node_y] + \")\"\n",
    "                summary_edge += e + \", \"\n",
    "        \n",
    "        summary_node = summary_node[:-2] if summary_node else \"\"\n",
    "        summary_edge = summary_edge[:-2] if summary_edge else \"\"\n",
    "        \n",
    "        return patient_id, {\n",
    "            \"entities\": patient_entities,\n",
    "            \"summary_entity\": summary_entity,\n",
    "            \"summary_node\": summary_node,\n",
    "            \"summary_edge\": summary_edge\n",
    "        }\n",
    "\n",
    "    # Process patients in parallel\n",
    "    entities = defaultdict(list)\n",
    "    summary_entities = defaultdict(str)\n",
    "    summary_nodes = defaultdict(str)\n",
    "    summary_edges = defaultdict(str)\n",
    "    \n",
    "    print(f\"Processing {mode} data with multithreading...\")\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        results = list(tqdm(\n",
    "            executor.map(process_patient, patients),\n",
    "            total=len(patients),\n",
    "            desc=f\"Processing {mode} data\"\n",
    "        ))\n",
    "\n",
    "    # Collect results\n",
    "    for patient_id, result in results:\n",
    "        entities[patient_id] = result[\"entities\"]\n",
    "        summary_entities[patient_id] = result[\"summary_entity\"]\n",
    "        summary_nodes[patient_id] = result[\"summary_node\"] \n",
    "        summary_edges[patient_id] = result[\"summary_edge\"]\n",
    "\n",
    "    # def get_summary(p):\n",
    "    #     return create_summary(notes=notes_extract[p],\n",
    "    #                          ehr=summary_entities[p],\n",
    "    #                          nodes=summary_nodes[p],\n",
    "    #                          edges=summary_edges[p],\n",
    "    #                          )\n",
    "    # summaries = {}\n",
    "    # def worker_summary(pid):\n",
    "    #     return pid, get_summary(pid)\n",
    "    # progress_bar = tqdm(total=len(patients), desc=\"Generating summaries\", unit=\"pt\")\n",
    "    # with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    #     future_to_pid = {executor.submit(worker_summary, pid): pid for pid in patients}\n",
    "    #     for future in as_completed(future_to_pid):\n",
    "    #         pid, summary = future.result()\n",
    "    #         summaries[pid] = summary\n",
    "    #         progress_bar.update(1)\n",
    "    # progress_bar.close()\n",
    "\n",
    "    def summary_for_pid(pid):\n",
    "        return pid, create_summary(\n",
    "            notes=notes_extract[pid],\n",
    "            ehr=summary_entities[pid],\n",
    "            nodes=summary_nodes[pid],\n",
    "            edges=summary_edges[pid],\n",
    "        )\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=10) as ex:\n",
    "        summaries = dict(\n",
    "            tqdm(ex.map(summary_for_pid, patients),\n",
    "                total=len(patients),\n",
    "                desc=\"Generating summaries\",\n",
    "                unit=\"pt\")\n",
    "        )\n",
    "\n",
    "    text_dtype = h5py.string_dtype(encoding=\"utf-8\")\n",
    "    with h5py.File(h5_path, \"a\") as h5:\n",
    "        for pid, summary in tqdm(summaries.items(), desc=\"Writing HDF5\", unit=\"pt\"):\n",
    "            gname = str(pid)\n",
    "            if gname in h5:\n",
    "                del h5[gname]\n",
    "            grp = h5.create_group(gname)\n",
    "            grp.create_dataset(\"PatientID\", data=np.asarray(pid, dtype=\"int64\"))\n",
    "            grp.create_dataset(\"SummaryText\", data=np.asarray(summary, dtype=object), dtype=text_dtype)\n",
    "            grp.create_dataset(\"SummaryEmbedding\", data=langchain_chunk_embed(summary) if USE_CHUNKING \n",
    "                               else plain_truncate(text=summary, max_length=256), compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8462f372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train data with multithreading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train data:   0%|          | 0/11200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 44133 embeddings from D:/Lab/Research/EMERGE-REPLICATE/good_data/curated/disease_features_cleaned.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Processing train data: 100%|██████████| 11200/11200 [45:00<00:00,  4.15it/s] \n",
      "Generating summaries:   0%|          | 0/11200 [18:38<?, ?pt/s]\n"
     ]
    },
    {
     "ename": "OllamaError",
     "evalue": "HTTP error: 504 Server Error: Gateway Time-out for url: http://192.168.100.205:11434/api/generate",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Lab\\Research\\EMERGE-REPLICATE\\utils\\call_llm.py:25\u001b[39m, in \u001b[36mollama_completion_with_backoff\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m     24\u001b[39m response = requests.post(BAILAB_HTTP, json=kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m json.loads(response.text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Lab\\Research\\EMERGE-REPLICATE\\.venv\\Lib\\site-packages\\requests\\models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 504 Server Error: Gateway Time-out for url: http://192.168.100.205:11434/api/generate",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mOllamaError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(SUMEMB_TRAIN) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(SUMEMB_VAL) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(SUMEMB_TEST):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mcreate_summary_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m     create_summary_embeddings(\u001b[33m\"\u001b[39m\u001b[33mval\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m     create_summary_embeddings(\u001b[33m\"\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 142\u001b[39m, in \u001b[36mcreate_summary_embeddings\u001b[39m\u001b[34m(mode)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pid, create_summary(\n\u001b[32m    135\u001b[39m         notes=notes_extract[pid],\n\u001b[32m    136\u001b[39m         ehr=summary_entities[pid],\n\u001b[32m    137\u001b[39m         nodes=summary_nodes[pid],\n\u001b[32m    138\u001b[39m         edges=summary_edges[pid],\n\u001b[32m    139\u001b[39m     )\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor(max_workers=\u001b[32m10\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     summaries = \u001b[38;5;28mdict\u001b[39m(\n\u001b[32m    143\u001b[39m         tqdm(ex.map(summary_for_pid, patients),\n\u001b[32m    144\u001b[39m             total=\u001b[38;5;28mlen\u001b[39m(patients),\n\u001b[32m    145\u001b[39m             desc=\u001b[33m\"\u001b[39m\u001b[33mGenerating summaries\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    146\u001b[39m             unit=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    147\u001b[39m     )\n\u001b[32m    149\u001b[39m text_dtype = h5py.string_dtype(encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m h5py.File(h5_path, \u001b[33m\"\u001b[39m\u001b[33ma\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m h5:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Lab\\Research\\EMERGE-REPLICATE\\.venv\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py:619\u001b[39m, in \u001b[36mExecutor.map.<locals>.result_iterator\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[32m    617\u001b[39m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[32m    618\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m619\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    620\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    621\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs.pop(), end_time - time.monotonic())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py:317\u001b[39m, in \u001b[36m_result_or_cancel\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    316\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    319\u001b[39m         fut.cancel()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py:456\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    458\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\thread.py:58\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     60\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 134\u001b[39m, in \u001b[36mcreate_summary_embeddings.<locals>.summary_for_pid\u001b[39m\u001b[34m(pid)\u001b[39m\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msummary_for_pid\u001b[39m(pid):\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pid, \u001b[43mcreate_summary\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnotes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnotes_extract\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m        \u001b[49m\u001b[43mehr\u001b[49m\u001b[43m=\u001b[49m\u001b[43msummary_entities\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43msummary_nodes\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m        \u001b[49m\u001b[43medges\u001b[49m\u001b[43m=\u001b[49m\u001b[43msummary_edges\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Lab\\Research\\EMERGE-REPLICATE\\utils\\call_llm.py:94\u001b[39m, in \u001b[36mcreate_summary\u001b[39m\u001b[34m(ehr, notes, nodes, edges, llm)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_summary\u001b[39m(ehr, notes, nodes, edges, llm = \u001b[33m\"\u001b[39m\u001b[33mqwen2.5:7b-instruct\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m     93\u001b[39m     prompt = summary_prompt_tmpl.format(ehr=ehr, notes=notes, nodes=nodes, edges=edges)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     response = \u001b[43mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_level\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlow\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m     \u001b[38;5;66;03m# log_to_file(LLM_LOG, f\"=== Prompt ===\\nModel: {llm}\\n{prompt}\\n=== Response ===\\n{response}\\n\", show_time=True)\u001b[39;00m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response \u001b[38;5;28;01mif\u001b[39;00m response \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mNo summary generated.\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Lab\\Research\\EMERGE-REPLICATE\\utils\\call_llm.py:68\u001b[39m, in \u001b[36mask\u001b[39m\u001b[34m(user_prompt, sys_prompt, model_name, max_tokens, temperature, reasoning_level)\u001b[39m\n\u001b[32m     65\u001b[39m     params[\u001b[33m\"\u001b[39m\u001b[33moptions\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m] = reasoning_level\n\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# Make the API call\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m response = \u001b[43mollama_completion_with_backoff\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# Extract and clean response\u001b[39;00m\n\u001b[32m     71\u001b[39m result = response.get(\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Lab\\Research\\EMERGE-REPLICATE\\.venv\\Lib\\site-packages\\backoff\\_sync.py:105\u001b[39m, in \u001b[36mretry_exception.<locals>.retry\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     96\u001b[39m details = {\n\u001b[32m     97\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtarget\u001b[39m\u001b[33m\"\u001b[39m: target,\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33margs\u001b[39m\u001b[33m\"\u001b[39m: args,\n\u001b[32m   (...)\u001b[39m\u001b[32m    101\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33melapsed\u001b[39m\u001b[33m\"\u001b[39m: elapsed,\n\u001b[32m    102\u001b[39m }\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     ret = \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m exception \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    107\u001b[39m     max_tries_exceeded = (tries == max_tries_value)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Lab\\Research\\EMERGE-REPLICATE\\utils\\call_llm.py:28\u001b[39m, in \u001b[36mollama_completion_with_backoff\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m json.loads(response.text)\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m requests.exceptions.HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m OllamaError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mHTTP error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m json.JSONDecodeError:\n\u001b[32m     30\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m OllamaError(\u001b[33m\"\u001b[39m\u001b[33mInvalid JSON response\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mOllamaError\u001b[39m: HTTP error: 504 Server Error: Gateway Time-out for url: http://192.168.100.205:11434/api/generate"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(SUMEMB_TRAIN) or not os.path.exists(SUMEMB_VAL) or not os.path.exists(SUMEMB_TEST):\n",
    "    create_summary_embeddings(\"train\")\n",
    "    create_summary_embeddings(\"val\")\n",
    "    create_summary_embeddings(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08db9c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = {}\n",
    "def load_summary_embeddings(h5_path):\n",
    "    with h5py.File(h5_path, \"r\") as h5:\n",
    "        for gname, grp in h5.items():\n",
    "            summaries[int(gname)] = {\n",
    "                \"PatientID\": int(grp[\"PatientID\"][()]),\n",
    "                \"SummaryText\": grp[\"SummaryText\"].asstr()[()],\n",
    "                \"SummaryEmbedding\": grp[\"SummaryEmbedding\"][()]\n",
    "            }\n",
    "\n",
    "def create_dataset(mode = \"train\"):\n",
    "    if mode not in [\"train\", \"val\", \"test\"]:\n",
    "        raise ValueError(\"mode must be 'train', 'val', or 'test'\")\n",
    "    if mode == \"train\":\n",
    "        df = pd.read_csv(TRAIN_DRAFT, encoding=\"utf-8\", low_memory=False)\n",
    "        h5_path = TRAIN\n",
    "        load_summary_embeddings(SUMEMB_TRAIN)\n",
    "    elif mode == \"val\":\n",
    "        df = pd.read_csv(VAL_DRAFT, encoding=\"utf-8\", low_memory=False)\n",
    "        h5_path = VAL\n",
    "        load_summary_embeddings(SUMEMB_VAL)\n",
    "    else:\n",
    "        df = pd.read_csv(TEST_DRAFT, encoding=\"utf-8\", low_memory=False)\n",
    "        h5_path = TEST\n",
    "        load_summary_embeddings(SUMEMB_TEST)\n",
    "\n",
    "    with h5py.File(h5_path, \"w\") as f:\n",
    "        pass\n",
    "\n",
    "    patients = list(df[\"PatientID\"].unique())\n",
    "    feature_cols = [c for c in df.columns if c not in [\"PatientID\",\"Outcome\",\"Readmission\"]]\n",
    "    target_map = df.groupby(\"PatientID\")[[\"Outcome\",\"Readmission\"]].first()\n",
    "\n",
    "    for p in tqdm(patients, total=len(patients), desc=f\"Storing {mode} data to HDF5\"):\n",
    "        data_ehr = df.loc[df[\"PatientID\"] == p, feature_cols].to_numpy()\n",
    "        data_notes = notes_emb[p]\n",
    "        data_summary = data_notes # fallback to notes embedding if summary missing\n",
    "        # data_summary = summaries[p][\"SummaryEmbedding\"]\n",
    "        outcome, readm = target_map.loc[p].astype(int)\n",
    "        data_target = (int(outcome), int(readm))\n",
    "        store_patient(h5_path, p, data_ehr, data_target, data_notes, data_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd1a876",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(TRAIN) or not os.path.exists(VAL) or not os.path.exists(TEST):\n",
    "    create_dataset(mode=\"train\")\n",
    "    create_dataset(mode=\"val\")\n",
    "    create_dataset(mode=\"test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
