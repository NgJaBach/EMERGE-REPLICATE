{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f2bda56",
   "metadata": {},
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17919e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "#\n",
    "from utils.bgem3 import cosine_filter, batch_encode\n",
    "from utils.call_llm import extract_note, create_summary\n",
    "from utils.clinical_longformer import langchain_chunk_embed, plain_truncate\n",
    "from utils.constants import *\n",
    "#\n",
    "import torch\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "#\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict, Counter\n",
    "import pickle\n",
    "import h5py\n",
    "#\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a38ff03",
   "metadata": {},
   "source": [
    "# Part 3: Entity Extraction and Create Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f891df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cols_between(_df, start_label, end_label=None):\n",
    "    cols = _df.columns\n",
    "    start_idx = cols.get_loc(start_label)\n",
    "    end_idx = len(cols) - 1 if end_label is None else cols.get_loc(end_label)\n",
    "    if start_idx > end_idx:\n",
    "        raise ValueError(f\"{start_label!r} comes after {end_label!r} in columns\")\n",
    "    return cols[start_idx:end_idx + 1]\n",
    "\n",
    "with open(DISEASE_FEATURES, \"rb\") as f:\n",
    "    kg = pickle.load(f)\n",
    "mapping = dict(zip(kg[\"node_index\"], kg[\"mondo_name\"]))\n",
    "\n",
    "with open(KG_ADJACENCY, \"rb\") as f:\n",
    "    adj = pickle.load(f)\n",
    "\n",
    "notes_emb = {}\n",
    "notes_extract = {}\n",
    "with h5py.File(NOTES_EMBEDDINGS, \"r\") as h5:\n",
    "    for patient_id in h5.keys():  # each group is named by patient_id\n",
    "        grp = h5[patient_id]\n",
    "        pid = int(grp[\"PatientID\"][()])\n",
    "        notes_emb[pid] = np.array(grp[\"Note\"])\n",
    "        notes_extract[pid] = np.array(grp[\"Extract\"]).astype(str)\n",
    "# print(len(notes_emb.keys()))\n",
    "# print(notes_emb[100].shape) # 768\n",
    "\n",
    "def _to_float32_array(x):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x.detach().cpu().float().numpy()\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return x.astype(\"float32\", copy=False)\n",
    "    raise TypeError(f\"Expected tensor/ndarray, got {type(x)}\")\n",
    "\n",
    "def store_patient(h5_path, p, ehr, target, notes, summary):\n",
    "    with h5py.File(h5_path, \"a\") as h5:\n",
    "        grp = h5.create_group(str(p))\n",
    "        grp.create_dataset(\"PatientID\", data=np.asarray(p, dtype=\"int64\"))\n",
    "        grp.create_dataset(\"X\", data=ehr, compression=\"gzip\")\n",
    "        grp.create_dataset(\"Note\", data=_to_float32_array(notes), compression=\"gzip\")\n",
    "        grp.create_dataset(\"Summary\", data=_to_float32_array(summary), compression=\"gzip\")\n",
    "        grp.create_dataset(\"Y\", data=np.asarray(target, dtype=\"int8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b5dc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary_embeddings(mode = \"train\"):\n",
    "    if mode not in [\"train\", \"val\", \"test\"]:\n",
    "        raise ValueError(\"mode must be 'train', 'val', or 'test'\")\n",
    "    if mode == \"train\":\n",
    "        df = pd.read_csv(TRAIN_DRAFT, encoding=\"utf-8\", low_memory=False)\n",
    "        h5_path = SUMEMB_TRAIN\n",
    "    elif mode == \"val\":\n",
    "        df = pd.read_csv(VAL_DRAFT, encoding=\"utf-8\", low_memory=False)\n",
    "        h5_path = SUMEMB_VAL\n",
    "    else:\n",
    "        df = pd.read_csv(TEST_DRAFT, encoding=\"utf-8\", low_memory=False)\n",
    "        h5_path = SUMEMB_TEST\n",
    "\n",
    "    with h5py.File(h5_path, \"w\") as f:\n",
    "        pass\n",
    "    \n",
    "    cat_cols = list(cols_between(\n",
    "        df,\n",
    "        \"Capillary refill rate->0.0\",\n",
    "        \"Glascow coma scale verbal response->3 Inapprop words\"\n",
    "    ))\n",
    "    num_cols = list(cols_between(df, \"Diastolic blood pressure\", None))\n",
    "\n",
    "    patients = list(df[\"PatientID\"].unique())\n",
    "    col_mean = df[num_cols].mean()\n",
    "    col_std = df[num_cols].std()\n",
    "\n",
    "    preprocess_patients = {}\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=f\"Processing {mode} data\"):\n",
    "        PatientID = row[\"PatientID\"]\n",
    "        entities = []\n",
    "\n",
    "        # record = \"\"\n",
    "        # if row[\"Sex\"] == 1:\n",
    "        #     record += \"Gender: Male\\n\"\n",
    "        # else:\n",
    "        #     record += \"Gender: Female\\n\"\n",
    "        # record += f\"Age: {row['Age']}\\n\"\n",
    "        \n",
    "        for c in cat_cols:\n",
    "            if row[c] == 1:\n",
    "                cat = c\n",
    "                if \"Glascow coma scale total\" not in cat:\n",
    "                    for i in range(0, 30, 1):\n",
    "                        cat = cat.replace(f\"->{i}.0\", \" : \")\n",
    "                        cat = cat.replace(f\"->{i}\", \" : \")\n",
    "                cat = cat.replace(\"->\", \" : \")\n",
    "                entities.append(cat)\n",
    "        \n",
    "        for c in num_cols:\n",
    "            if math.isnan(row[c]):\n",
    "                continue\n",
    "            z_score = (row[c] - col_mean[c]) / col_std[c]\n",
    "            if z_score > 2:\n",
    "                entities.append(f\"{c} too high\")\n",
    "            elif z_score < -2:\n",
    "                entities.append(f\"{c} too low\")\n",
    "\n",
    "        entities = list(set(entities))\n",
    "        summary_entities = \"\"\n",
    "        summary_nodes = \"\"\n",
    "        summary_edges = \"\"\n",
    "        nodes = []\n",
    "        for e in entities:\n",
    "            summary_entities += e + \", \"\n",
    "            idx = cosine_filter(None, e, threshold=0.6, top_k=3)\n",
    "            nodes.extend(idx)\n",
    "        summary_entities = summary_entities[:-2]\n",
    "\n",
    "        nodes = list(set(nodes))\n",
    "        \n",
    "        for n in nodes:\n",
    "            summary_nodes += kg.iloc[n][\"Diseases\"] + \", \"\n",
    "            node_x = kg.iloc[n][\"node_index\"]\n",
    "            for connect_to in adj[n]:\n",
    "                rela = connect_to[1]\n",
    "                node_y = connect_to[0]\n",
    "                if node_y not in kg[\"node_index\"].values:\n",
    "                    continue\n",
    "                e = \"(\" + mapping[node_x] + \", \" + str(rela) + \", \" + mapping[node_y] + \")\"\n",
    "                # print(e)\n",
    "                summary_edges += e + \", \"\n",
    "\n",
    "        summary_edges = summary_edges[:-2]\n",
    "        summary_nodes = summary_nodes[:-2]\n",
    "        summary_notes = notes_extract.get(PatientID, \"\")\n",
    "\n",
    "        preprocess_patients[PatientID] = {\n",
    "            \"notes\": summary_notes,\n",
    "            \"ehr\": summary_entities,\n",
    "            \"nodes\": summary_nodes,\n",
    "            \"edges\": summary_edges,\n",
    "        }\n",
    "    \n",
    "    def get_summary(p):\n",
    "        return create_summary(notes=preprocess_patients[p][\"notes\"],\n",
    "                              ehr=preprocess_patients[p][\"ehr\"],\n",
    "                              nodes=preprocess_patients[p][\"nodes\"],\n",
    "                              edges=preprocess_patients[p][\"edges\"],\n",
    "                              )\n",
    "\n",
    "    summaries = {}\n",
    "    def worker_summary(pid):\n",
    "        return pid, get_summary(pid)\n",
    "    progress_bar = tqdm(total=len(patients), desc=\"Generating summaries\", unit=\"pt\")\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        future_to_pid = {executor.submit(worker_summary, pid): pid for pid in patients}\n",
    "        for future in as_completed(future_to_pid):\n",
    "            pid, summary = future.result()\n",
    "            if summary is not None:\n",
    "                summaries[pid] = summary\n",
    "            progress_bar.update(1)\n",
    "    progress_bar.close()\n",
    "\n",
    "    text_dtype = h5py.string_dtype(encoding=\"utf-8\")\n",
    "    with h5py.File(h5_path, \"a\") as h5:\n",
    "        for pid, summary in tqdm(summaries.items(), desc=\"Writing HDF5\", unit=\"pt\"):\n",
    "            gname = str(pid)\n",
    "            if gname in h5:\n",
    "                del h5[gname]\n",
    "            grp = h5.create_group(gname)\n",
    "            grp.create_dataset(\"PatientID\", data=np.asarray(pid, dtype=\"int64\"))\n",
    "            grp.create_dataset(\"SummaryText\", data=np.asarray(summary, dtype=object), dtype=text_dtype)\n",
    "            grp.create_dataset(\"SummaryEmbedding\", data=langchain_chunk_embed(summary) if USE_CHUNKING else plain_truncate(text=summary, max_length=256), compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6f0b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORKERS = 1\n",
    "\n",
    "def create_summary_embeddings(mode = \"train\"):\n",
    "    if mode not in [\"train\", \"val\", \"test\"]:\n",
    "        raise ValueError(\"mode must be 'train', 'val', or 'test'\")\n",
    "    if mode == \"train\":\n",
    "        df = pd.read_csv(TRAIN_DRAFT, encoding=\"utf-8\", low_memory=False)\n",
    "        h5_path = SUMEMB_TRAIN\n",
    "    elif mode == \"val\":\n",
    "        df = pd.read_csv(VAL_DRAFT, encoding=\"utf-8\", low_memory=False)\n",
    "        h5_path = SUMEMB_VAL\n",
    "    else:\n",
    "        df = pd.read_csv(TEST_DRAFT, encoding=\"utf-8\", low_memory=False)\n",
    "        h5_path = SUMEMB_TEST\n",
    "\n",
    "    with h5py.File(h5_path, \"w\") as f:\n",
    "        pass\n",
    "    \n",
    "    cat_cols = list(cols_between(\n",
    "        df,\n",
    "        \"Capillary refill rate->0.0\",\n",
    "        \"Glascow coma scale verbal response->3 Inapprop words\"\n",
    "    ))\n",
    "    num_cols = list(cols_between(df, \"Diastolic blood pressure\", None))\n",
    "\n",
    "    col_mean = df[num_cols].mean()\n",
    "    col_std = df[num_cols].std()\n",
    "\n",
    "    entities = defaultdict(list)\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=f\"Processing {mode} data\"):\n",
    "        PatientID = row[\"PatientID\"]\n",
    "\n",
    "        # record = \"\"\n",
    "        # if row[\"Sex\"] == 1:\n",
    "        #     record += \"Gender: Male\\n\"\n",
    "        # else:\n",
    "        #     record += \"Gender: Female\\n\"\n",
    "        # record += f\"Age: {row['Age']}\\n\"\n",
    "        \n",
    "        for c in cat_cols:\n",
    "            if row[c] == 1:\n",
    "                cat = c\n",
    "                if \"Glascow coma scale total\" not in cat:\n",
    "                    for i in range(0, 30, 1):\n",
    "                        cat = cat.replace(f\"->{i}.0\", \" : \")\n",
    "                        cat = cat.replace(f\"->{i}\", \" : \")\n",
    "                cat = cat.replace(\"->\", \" : \")\n",
    "                entities[PatientID].append(cat)\n",
    "        \n",
    "        for c in num_cols:\n",
    "            if math.isnan(row[c]):\n",
    "                continue\n",
    "            z_score = (row[c] - col_mean[c]) / col_std[c]\n",
    "            if z_score > 2:\n",
    "                entities[PatientID].append(f\"{c} too high\")\n",
    "            elif z_score < -2:\n",
    "                entities[PatientID].append(f\"{c} too low\")\n",
    "\n",
    "    # Match entities to knowledge graph\n",
    "    patients = list(df[\"PatientID\"].unique())\n",
    "\n",
    "    def get_summary(p):\n",
    "        entities[p] = list(set(entities[p]))\n",
    "        summary_entities = \"\"\n",
    "        summary_nodes = \"\"\n",
    "        summary_edges = \"\"\n",
    "        nodes = []\n",
    "        for e in entities[p]:\n",
    "            summary_entities += e + \", \"\n",
    "            idx = cosine_filter(None, e, threshold=0.6, top_k=3)\n",
    "            nodes.extend(idx)\n",
    "        summary_entities = summary_entities[:-2]\n",
    "\n",
    "        nodes = list(set(nodes))\n",
    "        \n",
    "        for n in nodes:\n",
    "            summary_nodes += kg.iloc[n][\"Diseases\"] + \", \"\n",
    "            node_x = kg.iloc[n][\"node_index\"]\n",
    "            for connect_to in adj[n]:\n",
    "                rela = connect_to[1]\n",
    "                node_y = connect_to[0]\n",
    "                if node_y not in kg[\"node_index\"].values:\n",
    "                    continue\n",
    "                e = \"(\" + mapping[node_x] + \", \" + str(rela) + \", \" + mapping[node_y] + \")\"\n",
    "                # print(e)\n",
    "                summary_edges += e + \", \"\n",
    "\n",
    "        summary_edges = summary_edges[:-2]\n",
    "        summary_nodes = summary_nodes[:-2]\n",
    "        summary_notes = notes_extract.get(p, \"\")\n",
    "\n",
    "        return create_summary(notes=summary_notes,\n",
    "                              ehr=summary_entities,\n",
    "                              nodes=summary_nodes,\n",
    "                              edges=summary_edges,\n",
    "                              )\n",
    "\n",
    "    summaries = {}\n",
    "    def worker_summary(pid):\n",
    "        return pid, get_summary(pid)\n",
    "    progress_bar = tqdm(total=len(patients), desc=\"Generating summaries\", unit=\"pt\")\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        future_to_pid = {executor.submit(worker_summary, pid): pid for pid in patients}\n",
    "        for future in as_completed(future_to_pid):\n",
    "            pid, summary = future.result()\n",
    "            if summary is not None:\n",
    "                summaries[pid] = summary\n",
    "            progress_bar.update(1)\n",
    "    progress_bar.close()\n",
    "\n",
    "    text_dtype = h5py.string_dtype(encoding=\"utf-8\")\n",
    "    with h5py.File(h5_path, \"a\") as h5:\n",
    "        for pid, summary in tqdm(summaries.items(), desc=\"Writing HDF5\", unit=\"pt\"):\n",
    "            gname = str(pid)\n",
    "            if gname in h5:\n",
    "                del h5[gname]\n",
    "            grp = h5.create_group(gname)\n",
    "            grp.create_dataset(\"PatientID\", data=np.asarray(pid, dtype=\"int64\"))\n",
    "            grp.create_dataset(\"SummaryText\", data=np.asarray(summary, dtype=object), dtype=text_dtype)\n",
    "            grp.create_dataset(\"SummaryEmbedding\", data=langchain_chunk_embed(summary) if USE_CHUNKING else plain_truncate(text=summary, max_length=256), compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8462f372",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(SUMEMB_TRAIN) or not os.path.exists(SUMEMB_VAL) or not os.path.exists(SUMEMB_TEST):\n",
    "    create_summary_embeddings(\"train\")\n",
    "    create_summary_embeddings(\"val\")\n",
    "    create_summary_embeddings(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08db9c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = {}\n",
    "def load_summary_embeddings(h5_path):\n",
    "    with h5py.File(h5_path, \"r\") as h5:\n",
    "        for gname, grp in h5.items():\n",
    "            summaries[int(gname)] = {\n",
    "                \"PatientID\": int(grp[\"PatientID\"][()]),\n",
    "                \"SummaryText\": grp[\"SummaryText\"].asstr()[()],\n",
    "                \"SummaryEmbedding\": grp[\"SummaryEmbedding\"][()]\n",
    "            }\n",
    "\n",
    "def create_dataset(mode = \"train\"):\n",
    "    if mode not in [\"train\", \"val\", \"test\"]:\n",
    "        raise ValueError(\"mode must be 'train', 'val', or 'test'\")\n",
    "    if mode == \"train\":\n",
    "        df = pd.read_csv(TRAIN_DRAFT, encoding=\"utf-8\", low_memory=False)\n",
    "        h5_path = TRAIN\n",
    "        load_summary_embeddings(SUMEMB_TRAIN)\n",
    "    elif mode == \"val\":\n",
    "        df = pd.read_csv(VAL_DRAFT, encoding=\"utf-8\", low_memory=False)\n",
    "        h5_path = VAL\n",
    "        load_summary_embeddings(SUMEMB_VAL)\n",
    "    else:\n",
    "        df = pd.read_csv(TEST_DRAFT, encoding=\"utf-8\", low_memory=False)\n",
    "        h5_path = TEST\n",
    "        load_summary_embeddings(SUMEMB_TEST)\n",
    "\n",
    "    with h5py.File(h5_path, \"w\") as f:\n",
    "        pass\n",
    "\n",
    "    patients = list(df[\"PatientID\"].unique())\n",
    "    feature_cols = [c for c in df.columns if c not in [\"PatientID\",\"Outcome\",\"Readmission\"]]\n",
    "    target_map = df.groupby(\"PatientID\")[[\"Outcome\",\"Readmission\"]].first()\n",
    "\n",
    "    for p in tqdm(patients, total=len(patients), desc=f\"Storing {mode} data to HDF5\"):\n",
    "        data_ehr = df.loc[df[\"PatientID\"] == p, feature_cols].to_numpy()\n",
    "        data_notes = notes_emb[p]\n",
    "        data_summary = data_notes # fallback to notes embedding if summary missing\n",
    "        # data_summary = summaries[p][\"SummaryEmbedding\"]\n",
    "        outcome, readm = target_map.loc[p].astype(int)\n",
    "        data_target = (int(outcome), int(readm))\n",
    "        store_patient(h5_path, p, data_ehr, data_target, data_notes, data_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd1a876",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(TRAIN) or not os.path.exists(VAL) or not os.path.exists(TEST):\n",
    "    create_dataset(mode=\"train\")\n",
    "    create_dataset(mode=\"val\")\n",
    "    create_dataset(mode=\"test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
