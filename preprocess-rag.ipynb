{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f2bda56",
   "metadata": {},
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17919e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "#\n",
    "from utils.bgem3 import cosine_filter, batch_encode\n",
    "from utils.call_llm import extract_note, create_summary\n",
    "from utils.clinical_longformer import langchain_chunk_embed, plain_truncate\n",
    "from utils.constants import *\n",
    "#\n",
    "import torch\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "#\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict, Counter\n",
    "import pickle\n",
    "import h5py\n",
    "from contextlib import ExitStack\n",
    "#\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "def cols_between(_df, start_label, end_label=None):\n",
    "    cols = _df.columns\n",
    "    start_idx = cols.get_loc(start_label)\n",
    "    end_idx = len(cols) - 1 if end_label is None else cols.get_loc(end_label)\n",
    "    if start_idx > end_idx:\n",
    "        raise ValueError(f\"{start_label!r} comes after {end_label!r} in columns\")\n",
    "    return cols[start_idx:end_idx + 1]\n",
    "\n",
    "def _to_float32_array(x):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x.detach().cpu().float().numpy()\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return x.astype(\"float32\", copy=False)\n",
    "    raise TypeError(f\"Expected tensor/ndarray, got {type(x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a38ff03",
   "metadata": {},
   "source": [
    "# Part 3: Entity Extraction and Create Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f891df",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DISEASE_FEATURES, \"rb\") as f:\n",
    "    kg = pickle.load(f)\n",
    "mapping_disease = dict(zip(kg[\"node_index\"], kg[\"mondo_name\"]))\n",
    "\n",
    "with open(KG_ADJACENCY, \"rb\") as f:\n",
    "    adj = pickle.load(f)\n",
    "\n",
    "with open(NOTES_EXTRACTS, \"rb\") as f:\n",
    "    extracts = pickle.load(f)\n",
    "notes_extract = dict(zip(extracts[\"PatientID\"], extracts[\"Extract\"]))\n",
    "\n",
    "notes_emb = {}\n",
    "with h5py.File(NOTES_EMBEDDINGS, \"r\") as h5:\n",
    "    for patient_id in h5.keys():  # each group is named by patient_id\n",
    "        grp = h5[patient_id]\n",
    "        pid = int(grp[\"PatientID\"][()])\n",
    "        notes_emb[pid] = np.array(grp[\"Note\"])\n",
    "# print(len(notes_emb.keys()))\n",
    "# print(notes_emb[100].shape) # 768\n",
    "\n",
    "def store_patient(h5_path, p, ehr, target, notes, summary):\n",
    "    with h5py.File(h5_path, \"a\") as h5:\n",
    "        grp = h5.create_group(str(p))\n",
    "        grp.create_dataset(\"PatientID\", data=np.asarray(p, dtype=\"int64\"))\n",
    "        grp.create_dataset(\"X\", data=ehr, compression=\"gzip\")\n",
    "        grp.create_dataset(\"Note\", data=_to_float32_array(notes), compression=\"gzip\")\n",
    "        grp.create_dataset(\"Summary\", data=_to_float32_array(summary), compression=\"gzip\")\n",
    "        grp.create_dataset(\"Y\", data=np.asarray(target, dtype=\"int8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6f0b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = defaultdict(list)\n",
    "summary_entities = defaultdict(str)\n",
    "summary_nodes = defaultdict(str)\n",
    "summary_edges = defaultdict(str)\n",
    "\n",
    "def offload_computations(mode):\n",
    "    df = pd.read_csv(TRAIN_DRAFT, encoding=\"utf-8\", low_memory=False) if mode == \"train\" else (\n",
    "         pd.read_csv(VAL_DRAFT, encoding=\"utf-8\", low_memory=False) if mode == \"val\" else\n",
    "         pd.read_csv(TEST_DRAFT, encoding=\"utf-8\", low_memory=False)\n",
    "    )\n",
    "    \n",
    "    cat_cols = list(cols_between(\n",
    "        df,\n",
    "        \"Capillary refill rate->0.0\",\n",
    "        \"Glascow coma scale verbal response->3 Inapprop words\"\n",
    "    ))\n",
    "    num_cols = list(cols_between(df, \"Diastolic blood pressure\", None))\n",
    "\n",
    "    patients = list(df[\"PatientID\"].unique())\n",
    "    col_mean = df[num_cols].mean()\n",
    "    col_std = df[num_cols].std()\n",
    "\n",
    "    # Define a worker function to process a patient's data\n",
    "    def process_patient(patient_id):\n",
    "        patient_entities = []\n",
    "        patient_rows = df[df[\"PatientID\"] == patient_id]\n",
    "        \n",
    "        for _, row in patient_rows.iterrows():\n",
    "            # Process categorical columns\n",
    "            for c in cat_cols:\n",
    "                if row[c] == 1:\n",
    "                    cat = c\n",
    "                    if \"Glascow coma scale total\" not in cat:\n",
    "                        for i in range(0, 30, 1):\n",
    "                            cat = cat.replace(f\"->{i}.0\", \" : \")\n",
    "                            cat = cat.replace(f\"->{i}\", \" : \")\n",
    "                    cat = cat.replace(\"->\", \" : \")\n",
    "                    patient_entities.append(cat)\n",
    "            \n",
    "            # Process numerical columns\n",
    "            for c in num_cols:\n",
    "                if math.isnan(row[c]):\n",
    "                    continue\n",
    "                z_score = (row[c] - col_mean[c]) / col_std[c]\n",
    "                if z_score > 2:\n",
    "                    patient_entities.append(f\"{c} too high\")\n",
    "                elif z_score < -2:\n",
    "                    patient_entities.append(f\"{c} too low\")\n",
    "        \n",
    "        # Remove duplicates\n",
    "        patient_entities = list(set(patient_entities))\n",
    "        \n",
    "        # Create summary strings and find nodes\n",
    "        nodes = []\n",
    "        summary_entity = \"\"\n",
    "        for e in patient_entities:\n",
    "            summary_entity += e + \", \"\n",
    "            idx = cosine_filter(query=e, threshold=0.6, top_k=3)\n",
    "            nodes.extend(idx)\n",
    "        summary_entity = summary_entity[:-2] if summary_entity else \"\"\n",
    "        \n",
    "        # Remove duplicate nodes\n",
    "        nodes = list(set(nodes))\n",
    "        \n",
    "        # Process nodes and edges\n",
    "        summary_node = \"\"\n",
    "        summary_edge = \"\"\n",
    "        for n in nodes:\n",
    "            summary_node += kg.iloc[n][\"Diseases\"] + \", \"\n",
    "            node_x = kg.iloc[n][\"node_index\"]\n",
    "            for connect_to in adj[n]:\n",
    "                rela = connect_to[1]\n",
    "                node_y = connect_to[0]\n",
    "                if node_y not in kg[\"node_index\"].values:\n",
    "                    continue\n",
    "                e = \"(\" + mapping_disease[node_x] + \", \" + str(rela) + \", \" + mapping_disease[node_y] + \")\"\n",
    "                summary_edge += e + \", \"\n",
    "        \n",
    "        summary_node = summary_node[:-2] if summary_node else \"\"\n",
    "        summary_edge = summary_edge[:-2] if summary_edge else \"\"\n",
    "        \n",
    "        return patient_id, {\n",
    "            \"entities\": patient_entities,\n",
    "            \"summary_entity\": summary_entity,\n",
    "            \"summary_node\": summary_node,\n",
    "            \"summary_edge\": summary_edge\n",
    "        }\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        results = list(tqdm(\n",
    "            executor.map(process_patient, patients),\n",
    "            total=len(patients),\n",
    "            desc=f\"Processing {mode} data\"\n",
    "        ))\n",
    "\n",
    "    # Collect results\n",
    "    for patient_id, result in results:\n",
    "        entities[patient_id] = result[\"entities\"]\n",
    "        summary_entities[patient_id] = result[\"summary_entity\"]\n",
    "        summary_nodes[patient_id] = result[\"summary_node\"] \n",
    "        summary_edges[patient_id] = result[\"summary_edge\"]\n",
    "\n",
    "    return patients\n",
    "\n",
    "def create_summary_embeddings():\n",
    "    patients_train = offload_computations(\"train\")\n",
    "    patients_val = offload_computations(\"val\")\n",
    "    patients_test = offload_computations(\"test\")\n",
    "    patients = patients_train + patients_val + patients_test\n",
    "\n",
    "    def summary_for_pid(pid):\n",
    "        return pid, create_summary(\n",
    "            notes=notes_extract[pid],\n",
    "            ehr=summary_entities[pid],\n",
    "            nodes=summary_nodes[pid],\n",
    "            edges=summary_edges[pid],\n",
    "        )\n",
    "    with ThreadPoolExecutor(max_workers=10) as ex:\n",
    "        summaries = dict(\n",
    "            tqdm(ex.map(summary_for_pid, patients),\n",
    "                total=len(patients),\n",
    "                desc=\"Generating summaries\",\n",
    "                unit=\"pt\")\n",
    "        )\n",
    "\n",
    "    with h5py.File(SUMMARIES_EMBEDDINGS, \"w\") as f:\n",
    "        pass\n",
    "\n",
    "    text_dtype = h5py.string_dtype(encoding=\"utf-8\")\n",
    "    with h5py.File(SUMMARIES_EMBEDDINGS, \"a\") as h5:\n",
    "        for pid, summary in tqdm(summaries.items(), desc=\"Writing HDF5\", unit=\"pt\"):\n",
    "            gname = str(pid)\n",
    "            if gname in h5:\n",
    "                del h5[gname]\n",
    "            grp = h5.create_group(gname)\n",
    "            grp.create_dataset(\"PatientID\", data=np.asarray(pid, dtype=\"int64\"))\n",
    "            grp.create_dataset(\"SummaryText\", data=summary, dtype=text_dtype)\n",
    "            embedding = (\n",
    "                langchain_chunk_embed(summary) if USE_CHUNKING\n",
    "                else plain_truncate(text=summary, max_length=256)\n",
    "            )\n",
    "            grp.create_dataset(\"SummaryEmbedding\", data=embedding, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8462f372",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(SUMMARIES_EMBEDDINGS):\n",
    "    create_summary_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08db9c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = {}\n",
    "with h5py.File(SUMMARIES_EMBEDDINGS, \"r\") as h5:\n",
    "    for gname, grp in h5.items():\n",
    "        summaries[int(gname)] = {\n",
    "            \"PatientID\": int(grp[\"PatientID\"][()]),\n",
    "            \"SummaryText\": grp[\"SummaryText\"].asstr()[()],\n",
    "            \"SummaryEmbedding\": grp[\"SummaryEmbedding\"][()]\n",
    "        }\n",
    "\n",
    "def create_dataset(mode = \"train\"):\n",
    "    if mode not in [\"train\", \"val\", \"test\"]:\n",
    "        raise ValueError(\"mode must be 'train', 'val', or 'test'\")\n",
    "    if mode == \"train\":\n",
    "        df = pd.read_csv(TRAIN_DRAFT, encoding=\"utf-8\", low_memory=False)\n",
    "        h5_path = TRAIN\n",
    "    elif mode == \"val\":\n",
    "        df = pd.read_csv(VAL_DRAFT, encoding=\"utf-8\", low_memory=False)\n",
    "        h5_path = VAL\n",
    "    else:\n",
    "        df = pd.read_csv(TEST_DRAFT, encoding=\"utf-8\", low_memory=False)\n",
    "        h5_path = TEST\n",
    "\n",
    "    with h5py.File(h5_path, \"w\") as f:\n",
    "        pass\n",
    "\n",
    "    patients = list(df[\"PatientID\"].unique())\n",
    "    feature_cols = [c for c in df.columns if c not in [\"PatientID\",\"Outcome\",\"Readmission\"]]\n",
    "    target_map = df.groupby(\"PatientID\")[[\"Outcome\",\"Readmission\"]].first()\n",
    "\n",
    "    for p in tqdm(patients, total=len(patients), desc=f\"Storing {mode} data to HDF5\"):\n",
    "        data_ehr = df.loc[df[\"PatientID\"] == p, feature_cols].to_numpy()\n",
    "        data_notes = notes_emb[p]\n",
    "        data_summary = summaries[p][\"SummaryEmbedding\"]\n",
    "        outcome, readm = target_map.loc[p].astype(int)\n",
    "        data_target = (int(outcome), int(readm))\n",
    "        store_patient(h5_path, p, data_ehr, data_target, data_notes, data_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd1a876",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(TRAIN) or not os.path.exists(VAL) or not os.path.exists(TEST):\n",
    "    create_dataset(mode=\"train\")\n",
    "    create_dataset(mode=\"val\")\n",
    "    create_dataset(mode=\"test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
